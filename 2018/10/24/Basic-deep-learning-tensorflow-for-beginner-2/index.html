<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Databuzz, Databuzz.team@gmail.com"><title><Deep Learning> An introduction to Deep Learning with Tensorflow(Part-2) · Databuzz's Tech Blog</title><meta name="description" content="이번 포스트에서는 Tensorflow에서 꼭 알아야 할 기본적인 지식들에 대해서 알아보자.(Tensorflow설치법에 대해서는 다루지 않을 것)
1. Dataflow

출처 :Graphs and Sessions



이전 포스트 Part 1에서 이미 다룬 내용인데다 개"><meta name="keywords" content="DataScience"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/top-image.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/brands.css" integrity="sha384-rf1bqOAj3+pw6NqYrtaE1/4Se2NBwkIfeYbsFdtiR6TQz0acWiwJbv1IM/Nt/ite" crossorigin="anonymous"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/reset.css"><link rel="stylesheet" href="/css/new.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Luckiest+Guy|Stylish|Boogaloo"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/custom.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-127200932-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-127200932-1');
</script></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/main_logo_new.png" style="width:127px;"><h3 class="sidebar_title" title="">DataBuzz</h3><div class="description"><p class="title_description">Awesome Tech Blog</p></div></div></div><div class="featured-tags-wrap"><h1 class="featured-tags-title">FEATURED TAGS</h1><ul class="featured-tags-ul"><li class="featured-tags-li tag-li-detail"> Data Science </li><li class="featured-tags-li tag-li-detail"> Machine Learning </li><li class="featured-tags-li tag-li-detail"> Hexo </li><li class="featured-tags-li tag-li-detail"> Mathjax </li><li class="featured-tags-li tag-li-detail"> Git </li><li class="featured-tags-li tag-li-detail"> Tech News </li><li class="featured-tags-li tag-li-detail"> Artificial Intelligence </li><li class="featured-tags-li tag-li-detail"> Deep Learning </li><li class="featured-tags-li tag-li-detail"> Tensorflow </li></ul></div><div class="footer"><span>@ 2018 DataBuzz All Rights Reserved</span><br><span>Hexo Theme by </span><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><span>&nbsp;&nbsp;&nbsp; Edited by </span><a href="https://danialdaehyunnam.github.io"> Danial Nam </a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/"><i class="fas fa-home nav-icon"></i>Home</a></li><li><a href="/archives"><i class="fas fa-archive nav-icon"></i>Archives</a></li><li><a href="/tags-and-authors"><i class="fas fa-user-tag nav-icon"></i>Tags & Authors</a></li><li><a href="/about-us"><i class="fas fa-building nav-icon"></i>About Us</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left back-btn-icon" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/sun.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>&lt;Deep Learning&gt; An introduction to Deep Learning with Tensorflow(Part-2)</a></h3></div><div class="post-content post-detail"><p>이번 포스트에서는 Tensorflow에서 꼭 알아야 할 기본적인 지식들에 대해서 알아보자.(Tensorflow설치법에 대해서는 다루지 않을 것)</p>
<h3 id="1-Dataflow"><a href="#1-Dataflow" class="headerlink" title="1. Dataflow"></a>1. Dataflow</h3><div style="width:40%; float:left;">
<img src="https://www.tensorflow.org/images/tensors_flowing.gif">
<p style="text-align:center"><a href="https://www.tensorflow.org/guide/graphs" target="_blank" rel="noopener">출처 :Graphs and Sessions</a></p>
</div>

<div style="width: 60%; float:right; color: #333;">
<a href="https://databuzz-team.github.io/2018/10/22/Basic-deep-learning-tensorflow-for-beginner/">이전 포스트 Part 1</a>에서 이미 다룬 내용인데다 개념 설명이라 지루할 수도 있지만, Tensorflow를 사용하기 위해선 꼭 이해해야 하는 부분을 그림과 함께 짧게 설명하고자 하니, 이 부분을 꼭 읽어주시길 바란다.

옆의 그림은 <strong>Tensorflow</strong> 공식 홈페이지에 가면 나와있는 이미지 파일인데, 정말 직관적으로 Tensorflow의 <strong>Dataflow Graph(Node And Operation)</strong> 를 표현해냈다.

먼저 <strong>Tensor</strong> 는 옆에서 보이는 검은 라인이고(영어로는 Edge이지만, 라인이 내겐 더 직관적이라 그렇게 설명한 것), <strong>Operation</strong> 은 노드들, 그림에서 타원들을 의미한다. 즉, Tensor가 Operation으로 들어가서 해당 Operation에서 설정한 연산을 진행하고 다시 Tensor를 Output으로 내보내는 것이다.
<br>
<br>
<blockquote>필자가 이해한 바대로라면 Tensor나 Operation이라는 낯선 단어들을 사용해서 어렵게 느껴지지만 결국은 함수의 기능을 한다고 봐주면 되겠다. 차이점은 Graph는 선언이고 Session을 통해서 Run을 한다는 것</blockquote>
<br>
물론! 끝은 Output으로 값을 내보내는 것을 목적으로 하는 것은 아니다. 우리의 목적은 <strong>W 가중치</strong> 를 <strong>Update</strong> 하는 것이므로, 마지막에 우리가 <strong>Optimizer</strong> 의 변수로 설정한 <strong>W1, b1, W2, b2 ...</strong> 들이 <strong>Update</strong> 되는 것으로 <strong>Session.run()</strong> 이 종료된다.
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(task)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>

</div>

<div style="width: 100%; clear:both; color: #333;">
<h3>2. Operation의 name과 scope의 간략한 소개</h3>
본격적으로 코드에 대해 설명하기 전에 <strong>Debugging</strong> 에 도움이 되는 정보인 Operation name에 대해서 간략하게 살펴만 보자.
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "c"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미 사용된 이름은 자동으로 유니크화 시킨다.</span></span><br><span class="line">c_1 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scope는 접두사로 붙게되는데 나중에 설명할 Tensorboard에서 확인할 때 훨씬 편리하다.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scope 아래로는 경로로 계층을 표현한다.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">  c_4 = tf.constant(<span class="number">4</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/c_1"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_5 = tf.constant(<span class="number">5</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/inner_1/c"</span></span><br></pre></td></tr></table></figure>

<p style="text-align:center; margin:0"><a href="https://www.tensorflow.org/guide/graphs" target="_blank" rel="noopener">출처 :Graphs and Sessions</a></p>

</div>

<div style="width: 100%; clear:both; color: #333;">
<h3>3. Tensorflow 가장 많이 사용되는 함수들을 알아보자</h3>이 포스트에서는 필자가 많이 사용된다고 생각하는 가장 기본적인 함수들만 작성했는데, 이 외에도 거의 모든 수학 연산은 다 구현되어 있으니 자세한 API는 <a href="https://www.tensorflow.org/api_docs/python/" target="_blank" rel="noopener">링크</a>를 통해서 찾아보도록 하자.
<ul><li><h5>tf.placeholder()</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<strong>tf.placeholder()</strong> 는 머신러닝에서 무조건 사용하는 함수이며, 구조는 위의 코드에서 보듯 dtype, shape, name으로 이루어져있는데 핵심은 <strong>shape</strong> 이다.
<br>
<br>
<blockquote>다른 포스트에서 언급하겠지만 tensorflow에서 shape을 이해하는 것은 매우 중요하다.</blockquote>
<br>
예를 들어 집값을 예측하는 모델을 우리가 만들고 있고, 집의 <strong>Feature(특징)</strong> 는 <strong>rooms, is_riverside</strong> 로 이루어져 있다고 하자. 그리고 만약 우리가 최종적으로 사용할 <strong>Feature</strong> 의 수는 위에서 말한 두 가지면 충분하다고 결정했다고 본다면, <strong>Input</strong> 데이터의 <strong>shape</strong> 으로 <strong>[?, 2]</strong>, 즉 컬럼은 2 개로 결정을 한 상태라는 것이다. 하지만, 집의 개수, 즉 데이터의 개수는 많을수록 좋은 것이므로 변동의 여지가 언제나 있는 값일 뿐 아니라 최종적으로 우리가 새로운 데이터를 예측하려 할 때에도 변하는 값이라는 것이다.
<br>
<br>
그런 이유에서 <strong>tensorflow</strong> 에서는 <strong>placeholder</strong> 라는 함수를 제공하는 것이며, <strong>Feature(X)</strong> 와 <strong>Label(y)</strong> 은 <strong>placeholder</strong> 를 사용해서 넣어준다. 주의할 점은 <strong>sess.run()</strong> 시에 <strong>feed_dict</strong> 에 꼭 값을 직접 <strong>넣어(feed)</strong> 주어야 한다는 것.

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x_data = np.array([[<span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>]])</span><br><span class="line">y_data = np.array([[<span class="number">120000</span>], [<span class="number">100000</span>], [<span class="number">200000</span>]])</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">2</span>], name=<span class="string">"X"</span>)</span><br><span class="line"><span class="comment"># 이 예제에서는 각 집마다의 가격을 예측하는 것이므로, shape은 [None, 1]이 된 것.</span></span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"y"</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><h5>tf.Variable()</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</span><br></pre></td></tr></table></figure>

머신러닝을 통해서 구하고자하는 값인 <strong>Weight</strong> 나 <strong>Bias</strong> 와 같은 값은 <strong>tensorflow</strong> 의 <strong>tf.Variable()</strong> 함수를 사용해서 선언해야한다. 구조는 위의 code에서 보듯 아주 단순하며, 보통은 Random하게 초기화하는 경우가 많으므로 행렬곱을 할 상대인 <strong>X</strong> 와 예측 값으로 내보내는 <strong>y</strong> 의 <strong>shape</strong> 을 고려해서 <strong>tf.random_normal()</strong> 을 사용하게 된다.
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>]), name=<span class="string">'wight'</span>)</span><br><span class="line">b = tf.Variable(tf.random_normal([<span class="number">1</span>]), name=<span class="string">'bias'</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><h5>tf.matmul()</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.matmul(</span><br><span class="line">    a,</span><br><span class="line">    b,</span><br><span class="line">    transpose_a=<span class="keyword">False</span>,</span><br><span class="line">    transpose_b=<span class="keyword">False</span>,</span><br><span class="line">    adjoint_a=<span class="keyword">False</span>,</span><br><span class="line">    adjoint_b=<span class="keyword">False</span>,</span><br><span class="line">    a_is_sparse=<span class="keyword">False</span>,</span><br><span class="line">    b_is_sparse=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

머신러닝에서는 <strong>원소간의 곱(Element-wise multiplication)</strong> 보다는 <strong>행렬곱(Matrix multiplication)</strong> 이 훨씬 많이 쓰이므로 <strong>tf.matmul()</strong> 은 꼭 알아야하는 함수이다.
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hypothesis = tf.matmul(X, W) + b</span><br></pre></td></tr></table></figure>

</li>

<li><h5>tf.train module</h5>
오늘 소개할 마지막은 함수가 아닌 <strong>모듈(Module)</strong> 이다. 아래는 가장 보편적인 <strong>Optimizer</strong> 인 <strong>GradientDescentOptimizer</strong> 로 예를 들었지만, 훨씬 많은 모델들을 <strong>tensorflow</strong> 에서는 제공하고 있으니, 이 외에 필요한 정보는 <a href="https://www.tensorflow.org/api_docs/python/tf/train" target="_blank" rel="noopener">링크</a>에서 확인하도록 하자.
<br>
<br>
<blockquote><strong>목표 함수(Cost function)</strong> 에 대해서 이 포스트에서는 특별히 다루지 않지만, 다음 포스트들에서 CNN, RNN 등의 알고리즘을 구현하며 설명을 추가하겠다.
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cost = tf.reduce_mean(tf.square(hypothesis - y))</span><br><span class="line">train = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)</span><br></pre></td></tr></table></figure>

</li>
</ul>

<h3>4. 마무리</h3>

이번 포스트는 어려운 내용이 없지만 <strong>tensorflow</strong> 를 공부하며 간단한 모델들을 구현해보며 가장 자주 사용되고 중요하다고 느꼈던 점을 정리해보았는데, 처음 시작하는 사람들에게 꼭 도움이 되길 바란다.
<br>
<br>
아래는 가장 간단하게 회귀분석 모델을 구현한 코드이며, 위에서 설명한 개념들을 정말 간단한 예제이긴 하지만, 대략적으로 어떻게 쓰이나 보여주기 위해서 작성해보았다.
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x_data = np.array([[<span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>]])</span><br><span class="line">y_data = np.array([[<span class="number">120000</span>], [<span class="number">100000</span>], [<span class="number">200000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper parameter</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">n_epoch = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">2</span>], name=<span class="string">"X"</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>]), name=<span class="string">'wight'</span>)</span><br><span class="line">b = tf.Variable(tf.random_normal([<span class="number">1</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line">hypothesis = tf.matmul(X, W) + b</span><br><span class="line"></span><br><span class="line">cost = tf.reduce_mean(tf.square(hypothesis - y))</span><br><span class="line">train = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 변수가 있는 경우에는 초기화를 실행해줘야 한다.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># train이 반환하는 값은 우리에게 필요없다.</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(n_epoch):</span><br><span class="line">        c, _ = sess.run([cost, train], feed_dict=&#123;X: x_data, y: y_data&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Step :"</span>, step, <span class="string">"Cost :"</span>, c)</span><br><span class="line">            <span class="comment"># x, y를 임의로 만든거라..</span></span><br><span class="line">            <span class="comment"># 이 부분은 train data를 학습시키는지 확인하는 목적 외에는 없다.</span></span><br><span class="line">            print(sess.run(hypothesis, feed_dict=&#123;X: x_data&#125;))</span><br></pre></td></tr></table></figure>

</div>


<h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date tag-not-hover">2018-10-24</span><i class="fa fa-comment-o"></i><a href="/2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/#comments">Comments</a><!-- i.fas.fa-pencil-alt--><!-- a.tag(href=config.root + item.path, title= item.name)!= "Written by " + '<img class="post-footer-author" src="/images/avatar/' + png_name + '.png"> ' + item.name + " "--><a class="tag tag-not-hover">Written by <img class="post-footer-author" src="/images/avatar/danial-nam.png"> Danial Nam </a><br><i class="fa fa-tag"></i><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Machine Learning </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Artificial Intelligence </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Deep Learning </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Tensorflow </a></div></div></div></div><div class="share"></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2018/10/22/Basic-deep-learning-tensorflow-for-beginner/" title="&lt;Deep Learning&gt; An introduction to Deep Learning with Tensorflow(Part-1)">Next post</a></li></ul></div><a id="comments"></a><div id="disqus_thread"></div><script>var disqus_shortname = 'Databuzz';
var disqus_identifier = '2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/';
var disqus_title = '&lt;Deep Learning&gt; An introduction to Deep Learning with Tensorflow(Part-2)';
var disqus_url = 'https://databuzz-team.github.io/2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script></div></div></div></div><a href="#" id="back-to-top" title="Back to top"><i class="fas fa-angle-up"></i></a></body></html>