<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Databuzz&#39;s Tech Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://databuzz-team.github.io/"/>
  <updated>2018-11-18T16:39:53.558Z</updated>
  <id>https://databuzz-team.github.io/</id>
  
  <author>
    <name>Databuzz, Databuzz.team@gmail.com</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>&lt;Neural Network&gt; 인공신경망에 대한 이해(Part 2 - Back Propagation)</title>
    <link href="https://databuzz-team.github.io/2018/11/19/Back-Propagation-part2/"/>
    <id>https://databuzz-team.github.io/2018/11/19/Back-Propagation-part2/</id>
    <published>2018-11-18T15:24:13.000Z</published>
    <updated>2018-11-18T16:39:53.558Z</updated>
    
    <content type="html"><![CDATA[<div style="display: none;"><img src="/images/danial/back-prop/thumbnail.png"></div><p><br></p><p>이번 포스트(Part 2)에서는 인공 신경망을 가능하게 한 <strong>Back Propagation</strong> 에 대해 알아보도록 하겠다.(모바일에는 최적화되어있지 않으니 가능하면 PC로 보시길 추천한다)</p><p>만약 인공 신경망의 기본 개념과 <strong>Feedforward Propagation</strong> 에 대해서 잘 모른다면 <a href="https://Databuzz-team.github.io/2018/11/05/Back-Propagation/">이전 포스트(Part 1)</a>를 먼저 보고 오기 바란다.</p><h3 id="목차"><a href="#목차" class="headerlink" title="목차"></a>목차</h3><ul><li><a href="#why-back-propagation">왜 Back propagation를 이해해야 할까?</a></li><li><a href="#back-propagation">Back Propagation 설명</a></li></ul><h3 id="why-back-propagation" href="#why-back-propagation">왜 Back propagation를 이해해야 할까?</h3><p>“어차피 TensorFlow를 사용하면 다 자동으로 계산해주는 것인데 왜 우리가 공부해야 하는 것일까?”</p><p>합리적인 질문이다. 공식만 봐도 어려워보이는 이 부분을 공부하는 것이 동기부여가 쉽게 되지않는것이 사실이기 때문에..</p><p>하지만, <a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b" target="_blank" rel="noopener">Yes you should understand backprop</a> 글에서 설명했듯 이 <strong>Back Propagation</strong> 은 <strong>Leaky Abstraction</strong> 라는 것이다.</p><blockquote><p><strong>Leaky Abstraction</strong><br><br>Joel Spolsky이 설명한 <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" target="_blank" rel="noopener">The Law of Leaky Abstraction</a>에서 사용된 표현으로써 프로그래밍 언어를 추상화시켜서 내부 구현을 모르도록 만들어놨지만, <strong style="color:red">결국 제대로 사용하려면 내부 구현을 상당 부분 알아야 한다는 것을 의미한다.</strong></p></blockquote><p>이 포스트에서는 인공 신경망의 문제점에 대해서는 다루지 않기 때문에 왜 Leaky Abstraction이라 설명했는지 궁금한 경우에는 위의 블로그 링크를 참고하길 바란다.</p><h2 style="color: red;">이 포스트는 아직 작성 중에 있습니다. 최대한 빨리 작성해서 올리도록 하겠습니다!</h2><h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Artificial Intelligence" scheme="https://databuzz-team.github.io/tags/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://databuzz-team.github.io/tags/Deep-Learning/"/>
    
      <category term="Neural Network" scheme="https://databuzz-team.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Try StackEdit</title>
    <link href="https://databuzz-team.github.io/2018/11/05/howto-stackedit/"/>
    <id>https://databuzz-team.github.io/2018/11/05/howto-stackedit/</id>
    <published>2018-11-05T08:00:00.000Z</published>
    <updated>2018-11-08T15:25:25.530Z</updated>
    
    <content type="html"><![CDATA[<h1 id="StackEdit-사용하기"><a href="#StackEdit-사용하기" class="headerlink" title="StackEdit 사용하기"></a>StackEdit 사용하기</h1><p>이 글은 StackEdit으로 작성되었다.</p><p><img src="https://lh3.googleusercontent.com/PbFld4k-vlnfAeV61KRigj_0BDLKuseBt63P83AjlrAZT4akqk0OJgPFbGXti850KeYHLK2Fq6E" alt="welcome file"></p><p><strong>StackEdit</strong>(<a href="https://stackedit.io/app#" target="_blank" rel="noopener">https://stackedit.io/</a>)은 브라우저에서 마크다운을 읽고, 쓰고, 저장하고, 깃헙으로 발행할 수 있는 오픈소스 웹 어플리케이션.  작성하는 모든 내용이 실시간으로 저장, 동기화된다. 이미지, 인용, 코드, 표, 등등을 작성할 수 있고 LaTeX로 수학식을 쓰거나 다이어그램도 그릴 수 있다.<br>로컬에 저장되기 때문에 오프라인으로도 사용 가능.</p><p><a href="https://github.com/benweet/stackedit/" target="_blank" rel="noopener">개발자 깃헙</a>에 따르면 다음과 같은걸 할 수 있다. </p><h3 id="StackEdit은"><a href="#StackEdit은" class="headerlink" title="StackEdit은:"></a>StackEdit은:</h3><ul><li>여러 마크다운 파일을 온라인/오프라인으로 관리</li><li>파일을 마크다운, HTML, PDF, Word, EPUB 등으로 내보내기</li><li>클라우드에 마크다운파일을 동기화</li><li>구글 드라이브, 드롭박스, 로컬에 저장된 마크다운을 편집</li><li>마크다운을 GitHub, Gist, 구글 드라이브, 드롭박스에 발행</li><li>구글 드라이브와 CouchDB로 워크 스페이스를 공유<br><br><h3 id="주요-기능"><a href="#주요-기능" class="headerlink" title="주요 기능:"></a>주요 기능:</h3></li><li>편집창과 미리보기가 스크롤바를 묶어주는 스크롤 싱크로 실시간 HTML 프리뷰</li><li>Markdown Extra/GitHub Flavored Markdown,  Prism.js syntax highlighting 지원</li><li>KaTeX를 이용한 LaTeX 수식 구현</li><li>Mermaid를 이용한 다이어그램과 플로우차트</li><li><a href="https://www.froala.com/wysiwyg-editor/examples/toolbar-buttons" target="_blank" rel="noopener">WYSIWYG 컨트롤버튼</a> (스크린 사이즈에 맞춰 커스텀화되는 툴바 버튼</li><li>스마트 레이아웃</li><li>원클릭 동기화: Blogger, Dropbox, Gist, GitHub, Google Drive, WordPress, Zendesk</li></ul><p><br><br><br>아래 설명은 StackEdit을 시작하면 기본적으로 저장되어있는 Welcome File을 참고해 간단한 기능 소개를 하려 한다. 자세한 원리가 알고 싶다면 Welcome File에 친절한 설명과 깃헙 링크가 나와있다.</p><h2 id="동기화"><a href="#동기화" class="headerlink" title="동기화"></a>동기화</h2><p><img src="https://lh3.googleusercontent.com/Xv370o9zVCiqA3CNqCq8HBoo6siDtFpa5fQkKu4Aro_rVFYrah9iOVKycXyE-Uj-3_KoV8-kFHc" alt="" title="MENU"></p><p>로컬 브라우저에 저장되지만 <strong>구글계정</strong>으로 로그인 하면 다른 브라우저에서도 동기화가 가능하다. 파일을 별도로 관리하고 싶으면 <strong>구글 드라이브</strong> , <strong>드롭박스</strong>, <strong>깃헙</strong> 등에 워크 스페이스를 만들어 저장할 수 있고 모든 수정사항이 실시간으로 동기화된다. 오른쪽의 StackEdit아이콘을 누르면 메뉴가 나온다. 한 계정 안에서 여러개의 워크 스페이스를 만들 수 있음.</p><p><img src="https://lh3.googleusercontent.com/lDtQxjzRy6zfFRCiohVy10GOE8-WR7EmyDbWZj0Xgh7yUODkP1rNaUUOw8fa_k8Vygdn5aytSkI" alt="" title="Workspace"></p><p>사실 계정 하나로 로그인해놓고 브라우저에서 노트하는 걸로만 써와서 워크스페이스는 거의 안 써봤다. <br><br>깃에 바로 퍼블리싱 할 수 있는 기능은 굉장히 유용한데 한 가지 단점은 레포의 어느 폴더로 들어갈 지까지는 적용할 수 없어서 1)발행 후 2)폴더이동 의 작업이 두번씩 필요하다. 이렇게 하면 사실 동기화가 안 된다는게 아쉬운 부분.</p><h2 id="툴바"><a href="#툴바" class="headerlink" title="툴바"></a>툴바</h2><p><img src="https://lh3.googleusercontent.com/6nb7yQ5zbZeVVYHlm-4DLlkd_LgY9dd74lUnXnp_QzDv-IVN-Am0bip8j6JgJKNsac5Cgzj0t9gA" alt="enter image description here"><br><br><br>기본적인 기능은 다 툴바에 아이콘으로 표시되어있다. 순서대로 <strong>굵게</strong>, <strong>기울이기</strong>, <strong>글자 크기</strong>,<strong>취소선</strong>, <strong>인용</strong>,<strong>표</strong>,<strong>하이퍼링크</strong>,<strong>이미지</strong>. 굳이 아이콘을 이용하지 않아도 <strong>메뉴</strong>의 <strong>마크다운 치트시트</strong>에 친절하게 안내되어 있으니 참고하자.</p><h3 id="이미지-첨부하기"><a href="#이미지-첨부하기" class="headerlink" title="+ 이미지 첨부하기"></a>+ 이미지 첨부하기</h3><p><img src="https://lh3.googleusercontent.com/q--du9ULEm3AkmfX3VJDVk9OAQWMWd8zbNUFWNMhtIxU7MXuY_vx8bVu_v1FqfkU5rZDBLrAlOJ3" alt="enter image description here"><br><br> <br><br>이미지는 링크로 가져와야하는데 구글포토와 연동하면 구글포토에 업로드한 사진을 가져올 수 있다. 로컬에 있는 이미지를 올리고 싶을 때 구글포토 백업을 사용하면 편리하다. 가끔 이미지 사이즈가 멋대로 조절된다는 단점이 있다.<br> <br> <br> <br></p><h2 id="편집화면-미리보기"><a href="#편집화면-미리보기" class="headerlink" title="편집화면, 미리보기"></a>편집화면, 미리보기</h2><p><img src="https://lh3.googleusercontent.com/Fzjdt8B2-EecweJvJGZHR_cGzbGqSeM5inju_pJdMSnFqn4l9CxQfDnDt7y_q4NTtJvKb9Qjb2A-" alt="" title="control"></p><p><br>편집창 왼쪽에 보이는 아이콘 세 개다. 위에서부터 차례대로 <strong>툴바 접기/펴기</strong>, <strong>미리보기 열기 접기</strong>, <strong>미리보기</strong>.  </p><h3 id="1-미리보기"><a href="#1-미리보기" class="headerlink" title="1. 미리보기"></a>1. 미리보기</h3><p>편집한 내용을 미리보기 할 수 있다. 이 기능을 이용하면 StackEdit 내부의 기능을 사용하면서 프리젠테이션 용도로도 사용할 수 있을 듯.</p><p><img src="https://lh3.googleusercontent.com/0usMVJbbi4HpRL93oMhmq63j-IeS-Oe3rkglDLvDmBqUcff1DRX2IfcGbLUA2j8tqToCi-ur9kg" alt="" title="view only"></p><p><br></p><h3 id="2-미리보기-열기-접기"><a href="#2-미리보기-열기-접기" class="headerlink" title="2. 미리보기 열기 접기"></a>2. 미리보기 열기 접기</h3><p>미리보기를 열어서 편집과 미리보기 화면을 보면서 동시에 편집할 수 있다. 이미지를 띄우면 미리보기 창과 스크롤의 차이가 생긴다. <br><br><img src="https://lh3.googleusercontent.com/sb6GyTTWVV8kWDNt-Ni4nB9e_Co3SptQy0grevgGEXXk_bsfOIO5B1V8LWb7-0Z1M8cjbRC3JM8" alt="" title="twosides"></p><p><br><br><br><br>접으면 아래처럼 편집창만 보인다.<br></p><p><img src="https://lh3.googleusercontent.com/ORBPJTp_gxf7gA5WiXL2uzANpXLBhn3nCU5gh5P8wSBck17O9gOwjLOQFtLY24Dlqoe-038TqGQ" alt="" title="editonly"></p><h2 id="마크다운-확장기능"><a href="#마크다운-확장기능" class="headerlink" title="마크다운 확장기능"></a>마크다운 확장기능</h2><h2 id="SmartyPants"><a href="#SmartyPants" class="headerlink" title="SmartyPants"></a>SmartyPants</h2><p>표, 인용, 대쉬. 가장 유용하다.<br><br><br></p><div class="table-container"><table><thead><tr><th></th><th>ASCII</th><th>HTML</th></tr></thead><tbody><tr><td>Single backticks</td><td><code>&#39;Isn&#39;t this fun?&#39;</code></td><td>‘Isn’t this fun?’</td></tr><tr><td>Quotes</td><td><code>&quot;Isn&#39;t this fun?&quot;</code></td><td>“Isn’t this fun?”</td></tr><tr><td>Dashes</td><td><code>-- is en-dash, --- is em-dash</code></td><td>— is en-dash, —- is em-dash</td></tr></tbody></table></div><h2 id="KaTeX"><a href="#KaTeX" class="headerlink" title="KaTeX"></a>KaTeX</h2><p>KaTeX를 이용해 LaTeX의 수학식을 표시할 수 있다. 자세한건 Welcome File와 <a href="https://katex.org/" target="_blank" rel="noopener">KaTeX</a> 참조. 현재 블로그에서는 구현되지 않아서 이미지로 대체한다. </p><p>예:</p><p><img src="https://lh3.googleusercontent.com/B-hZCzfY5RXT5VpTh1S5EprLEhDARx32Lfqpgh5VUXXg3u8uuI7HCyAMq6gGbtjlorEdW7jLCggV=s10000" alt=""><br>코드: </p><p>The <em>Gamma function</em> satisfying $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$ is via the Euler integral</p><script type="math/tex; mode=display">\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.</script><h2 id="UML-다이어그램"><a href="#UML-다이어그램" class="headerlink" title="UML 다이어그램"></a>UML 다이어그램</h2><p>아래처럼 모델링을 위한 다이어그램을 그릴 수 있다. <br><br><img src="https://lh3.googleusercontent.com/2XhLOPDrgFGbxBNmtQ7pEJubtEs0UAV50PrAYDyColKnvFwKb0ww8f95r7RwaSzB-CTR4xRSQSj5" alt="enter image description here"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">Alice -&gt;&gt; Bob: Hello Bob, how are you?</span><br><span class="line">Bob--&gt;&gt;John: How about you John?</span><br><span class="line">Bob--x Alice: I am good thanks!</span><br><span class="line">Bob-x John: I am good thanks!</span><br><span class="line">Note right of John: Bob thinks a long&lt;br/&gt;long time, so long&lt;br/&gt;that the text does&lt;br/&gt;not fit on a row.</span><br><span class="line"></span><br><span class="line">Bob--&gt;Alice: Checking with John...</span><br><span class="line">Alice-&gt;John: Yes... John, how are you?</span><br></pre></td></tr></table></figure><p><br><br><br>플로우 차트는 아래처럼 그려진다.<br><img src="https://lh3.googleusercontent.com/noBNIQAm987yT5gHTSP52BUGgD1ZcaZkEMspO1mMLbLr-JkGO_ADmIDgqUOe2k9ed9T9NEsarF4S" alt="enter image description here"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[Square Rect] -- Link text --&gt; B((Circle))</span><br><span class="line">A --&gt; C(Round Rect)</span><br><span class="line">B --&gt; D&#123;Rhombus&#125;</span><br><span class="line">C --&gt; D</span><br></pre></td></tr></table></figure><p>[  ] 사각형<br>((  )) 원형<br>(   ) 모서리가 둥근 사각형<br>{   } 마름모</p><p>#<br>별도의 설치 없이 로컬 브라우저에서 편집할 수 있는 훌륭한 툴을 이용하는데에 도움이 되었길 바란다. 유용했다면 오픈소스에게 귀중한 5달러를 기부해도 좋을 듯. <br> <br> <br> <img src="https://lh3.googleusercontent.com/HjZDx-jC6U4n5_QOBp6fxxwt5IfjJ3NrNEgk4SwPmMXWb20YImZf1lll3MoNeALzEm2wLt9EaI3C" alt="" title="donate please"></p>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="SoHyun Bae" scheme="https://databuzz-team.github.io/categories/SoHyun-Bae/"/>
    
    
      <category term="Markdown" scheme="https://databuzz-team.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Neural Network&gt; 인공신경망에 대한 이해(Part 1 - Feedforward Propagation)</title>
    <link href="https://databuzz-team.github.io/2018/11/05/Back-Propagation/"/>
    <id>https://databuzz-team.github.io/2018/11/05/Back-Propagation/</id>
    <published>2018-11-05T05:16:52.000Z</published>
    <updated>2018-11-20T07:18:48.551Z</updated>
    
    <content type="html"><![CDATA[<p><br></p><p>이번 포스트(Part 1)에서는 TensorFlow로 DNN을 구현하기 전에 먼저 기본 개념을 알아보고 다음 포스트(Part 2)에서는 인공 신경망을 가능하게 한 <strong>Back Propagation</strong> 에 대해 알아보도록 하겠다.(모바일에는 최적화되어있지 않으니 가능하면 PC로 보시길 추천한다)</p><div style="display: none;"><img src="/images/danial/back-prop/thumbnail.png"></div><h3 id="목차"><a href="#목차" class="headerlink" title="목차"></a>목차</h3><ul><li><a href="#nn-history">Neural Network 역사 및 Back propagation의 중요성</a></li><li><a href="#feedforward-propagation">Feedforward Propagation 설명</a><ul><li><a href="#init-network">네트워크 초기화</a></li><li><a href="#layer1">Layer 1 (input -&gt; J)</a></li><li><a href="#layer2">Layer 2 (J -&gt; K)</a></li><li><a href="#layer3">Layer 3 (K -&gt; output)</a></li><li><a href="#conclusion">정리</a></li></ul></li><li><a href="#cost-function">오차 함수(Cost function)</a></li><li><a href="#optimization">가중치 최적화</a></li></ul><h3 id="nn-history" href="#nn-history">Neural Network 역사</h3><p> 1943년, 워런 맥컬록(Warren McCulloch)와 월터 피츠(Walter Pitts)의 수학과 임계 논리(Threshold logic)라 불리는 알고리즘을 바탕으로 신경망을 위한 계산한 모델이 만들어지며 신경망 연구의 초석을 닦으며 <strong>Neural Network</strong> 역사가 시작되었다.</p><p> 하지만, 1969년에 마빈 민스키(Marvin Minsky)와 시모어 페퍼트(Seymour Papert)에 의해 기계학습 논문이 발표된 후 침체되었는데, 그 이유는 두 가지였다.</p><ol><li><p><strong>단층 신경망</strong> 은 선형으로 분류하기 때문에 아래의 그림처럼 문제가 배타적 논리합 회로(XOR problem)인 경우에는 해결하지 못한다.</p><div><img style="width: 50%;" src="https://cdn-images-1.medium.com/max/2000/0*qdRb80zUpJPtrbRD."><a style="display: block; text-align: center;" href="https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b" target="_blank" rel="noopener">출처 : [Medium] Back-Propagation is very simple. Who made it Complicated ?</a></div></li><li><p>Computing power가 부족하다.</p></li></ol><p>위의 두 가지 문제점 외에도 가중치를 업데이트하기 위한 회로 수학 계산이 너무 복잡하였기에 <strong>오차역전파법(Back Propagation)</strong> 이 세상에 나오기 전까지는 연구가 침체될 수밖에 없었다.</p><h3 id="feedforward-propagation" href="#feedforward-propagation">Feedforward Propagation 설명</h3><h4 style="color: #cb4a44;" id="init-network" href="#init-network">네트워크 초기화</h4><img src="/images/danial/back-prop/network.png">$$Input = \left[ \begin{array}{cccc}i_{1} & i_{2} \\\end{array} \right]$$$$W_{ij} = \left[ \begin{array}{cccc}W_{i1j1} & W_{i1j2} & W_{i1j3} \\W_{i2j1} & W_{i2j2} & W_{i2j3} \\\end{array} \right]W_{jk} = \left[ \begin{array}{cccc}W_{j1k1} & W_{j1k2} & W_{j1k3} \\W_{j2k1} & W_{j2k2} & W_{j2k3} \\W_{j3k1} & W_{j3k2} & W_{j3k3} \\ \end{array} \right]W_{ko} = \left[ \begin{array}{cccc}W_{k1o1} & W_{k1o2} \\W_{k2o1} & W_{k2o2} \\W_{k3o1} & W_{k3o2} \\ \end{array} \right]$$$$Output = \left[ \begin{array}{cccc}o_{1} & o_{2} \\\end{array} \right]$$<p>이 포스트에서는 간단한 계산을 위해 Hidden layer가 2층 구조인 모형으로 초기화를 하였지만, 실제로는 layer마다 Neuron 개수나 Hidden layer의 층수는 우리가 알아내야 할 하이퍼 파라미터이다.</p><p><strong>인공 신경망</strong> 알고리즘에는 여느 예측 문제와 마찬가지로 Feature에 해당하는 Input이 존재하고, 들어온 데이터들이 Hidden layer의 Neuron이 가진 <strong>하이퍼 파라미터에 의해 모양이 바뀌는 비선형 기저 함수</strong></p><blockquote><strong>기저 함수</strong> 는 위에서 설명한 배타적 논리합 회로(XOR problem)를 해결하기 위한 방법으로, Input데이터 x대신 &phi;(x)를 사용하는 것을 의미한다.여기서 "<strong>하이퍼 파라미터에 의해 모양이 바뀌는</strong>"라는 수식어가 붙은 이유는 Hidden layer로 이동할 때 계산되는 <strong>행렬(Matrix)</strong> 과 <strong>Bias</strong> 에 의해서 계속해서 모양을 바뀌는 것을 의미한다. 가장 기본적인 <strong>Activation 함수</strong> 인 Logistic Sigmoid를 예로 보자면,$$Sigmoid = 1/(1+\mathrm{e}^{-(w_{ji}x + b_{j})})$$형태의 기저함수를 가지게 되는 것.</blockquote><p>에 의해서 비선형 문제를 해결할 수 있는 형태로 정보의 차원을 변경하며 전파하는 과정을 거친다. 마지막에는 Output으로 분류인 경우에는 Class 개수만큼, 회귀분석인 경우에는 1개의 실수값을 내보내는 것으로 마무리 된다.</p><p><strong>역방향 전파(Back Propagation)</strong> 을 이해하기 위해서는 먼저 인공신경망의 계산 과정인 <strong>순방향 전파(Feedforward propagation)</strong> 를 이해해야 하므로 살펴보도록 하자.</p><hr><h4 id="layer1" href="#layer1" style="color: #cb4a44; ">Layer 1 (Input -> J)</h4><div style="float:left; width:40%; margin-right: 10px;">  <img src="/images/danial/back-prop/layer_j.png"></div><div style="">$$\definecolor{first}{RGB}{10, 199, 139}\definecolor{second}{RGB}{74, 144, 226}\left[ \begin{array}{cccc}i_{1} & i_{2} \\\end{array} \right] \times\left[ \begin{array}{cccc}\textcolor{first}{W_{i1j1}} & \textcolor{first}{W_{i1j2}} & \textcolor{first}{W_{i1j3}} \\\textcolor{second}{W_{i2j1}} &\textcolor{second}{W_{i2j2}} &\textcolor{second}{W_{i2j3}} \\\end{array} \right] +\left[ \begin{array}{cccc}b_{j1} & b_{j2} & b_{j3} \\\end{array} \right]$$$$\downarrow$$$$\definecolor{first}{RGB}{10, 199, 139}\definecolor{second}{RGB}{74, 144, 226}\left[ \begin{array}{cccc}\textcolor{first}{W_{i1j1}} \times i_{1} +\textcolor{second}{W_{i2j1}} \times i_{2} + b_{j1} \\\textcolor{first}{W_{i1j2}} \times i_{1} +\textcolor{second}{W_{i2j2}} \times i_{2} +b_{j2} \\\textcolor{first}{W_{i1j3}} \times i_{1} +\textcolor{second}{W_{i2j3}} \times i_{2} +b_{j3} \\\end{array} \right]^{T} =\left[ \begin{array}{cccc}J_{in1} \\ J_{in2} \\ J_{in3} \\\end{array} \right]^{T}$$<p style="">  계산은 어려운게 없으니 따로 설명하지 않겠다. 중요한 점은 Hidden layer는 들어온 값(<strong>J<sub>in</sub></strong>)과 나가는 값(<strong>J<sub>out</sub></strong>)이 다르다는 것이다. 그 과정은 아래에서 살펴보자.</p></div><div class="clearfix" style="clear:both"><div style="float:left; width:40%; margin-right:10px;">  <img src="/images/danial/back-prop/activation_j.png"></div><div style=""><p style=""><strong>Hidden layer</strong>의 <strong>Neuron</strong>은 앞서 설명했듯 <strong>기저함수</strong>의 역할을 해야하므로 좌측의 그림처럼 <strong>J<sub>in</sub></strong>이 Activation function에 의해 변한 <strong>J<sub>out</sub></strong>을 다음 레이어에 들어가는 Input이 되게 한다.이 포스팅의 예제에서 <strong>활성화 함수(Activation function)</strong> 은 모두 <strong>Logistic Sigmoid</strong> 함수를 사용하기로 한다.</p>$$Sigmoid = 1/(1+\mathrm{e}^{-x})$$$$Sigmoid(J_{in}) =\left[ \begin{array}{cccc}1/(1+\mathrm{e}^{-J_{in1}}) \\1/(1+\mathrm{e}^{-J_{in2}}) \\1/(1+\mathrm{e}^{-J_{in3}}) \\\end{array} \right]^{T}= \left[ \begin{array}{cccc}J_{out1} \\ J_{out2} \\ J_{out3} \\\end{array} \right]^{T}$$<p style="">이제는 위에서 설명한 <strong>하이퍼 파라미터에 의해 모양이 바뀌는</strong>이라는 표현이 이해가 더 잘 될 것이다. <strong>J<sub>in</sub></strong>은 w<sub>ij</sub>와 b<sub>j</sub>에 의해 바뀌고 그에 의해 <strong>J<sub>out</sub></strong>이 바뀔 것이므로.</p></div></div><hr><div class="clearfix"><h4 id="layer2" href="#layer2" style="color: #cb4a44; clear:both; margin-top: 10px;">Layer 2 (J -> K)</h4><div style="float:left; width:40%; margin-right: 10px;">  <img src="/images/danial/back-prop/j_k_layer.png"></div><div style="">$$\definecolor{first}{RGB}{10, 233, 134}\definecolor{second}{RGB}{74, 144, 226}\definecolor{third}{RGB}{245, 166, 35}\left[ \begin{array}{cccc}J_{out1} & J_{out2} & J_{out3}\\\end{array} \right] \times\left[ \begin{array}{cccc}\textcolor{first}{W_{j1k1}} & \textcolor{first}{W_{j1k2}} & \textcolor{first}{W_{j1k3}} \\\textcolor{second}{W_{j2k1}} & \textcolor{second}{W_{j2k2}} & \textcolor{second}{W_{j2k3}} \\\textcolor{third}{W_{j3k1}} & \textcolor{third}{W_{j3k2}} & \textcolor{third}{W_{j3k3}} \\ \end{array} \right] +\left[ \begin{array}{cccc}b_{k1} & b_{k2} & b_{k3} \\\end{array} \right]$$$$\downarrow$$$$\definecolor{first}{RGB}{10, 233, 134}\definecolor{second}{RGB}{74, 144, 226}\definecolor{third}{RGB}{245, 166, 35}\left[ \begin{array}{cccc}\textcolor{first}{W_{j1k1}} \times J_{out1} +\textcolor{second}{W_{j2k1}} \times J_{out2} +\textcolor{third}{W_{j3k1}} \times J_{out3} + b_{k1}\\\textcolor{first}{W_{j1k2}} \times J_{out1} +\textcolor{second}{W_{j2k2}} \times J_{out2} +\textcolor{third}{W_{j3k2}} \times J_{out3} + b_{k2}\\\textcolor{first}{W_{j1k3}} \times J_{out1} +\textcolor{second}{W_{j2k3}} \times J_{out2} +\textcolor{third}{W_{j3k3}} \times J_{out3} + b_{k3}\\\end{array} \right]^{T} = \left[ \begin{array}{cccc}K_{in1} \\ K_{in2} \\ K_{in3} \\\end{array} \right]^{T}$$<p style="">  <strong>K<sub>in</sub></strong>과 <strong>K<sub>out</sub></strong> 사이의 Activation function은 Layer 1에서와 마찬가지로 Logistic Sigmoid를 사용한다.</p>$$Sigmoid(K_{in}) =\left[ \begin{array}{cccc}1/(1+\mathrm{e}^{-K_{in1}}) \\1/(1+\mathrm{e}^{-K_{in2}}) \\1/(1+\mathrm{e}^{-K_{in3}}) \\\end{array} \right]^{T}= \left[ \begin{array}{cccc}K_{out1} \\ K_{out2} \\ K_{out3} \\\end{array} \right]^{T}$$<p style="">  Layer 1과 비교해서 새로운 점이 없으므로 그림상에 Notation은 자세하게 하지 않았다.</p></div></div><hr><p></p><h4 id="layer3" href="#layer3" style="color: #cb4a44; clear:both; margin-top: 10px;">Layer 3 (K -&gt; output)</h4><p></p><div style="float:left; width:40%; margin-right: 10px;">  <img src="/images/danial/back-prop/k_o_layer.png"></div><div style="">$$\definecolor{first}{RGB}{245, 166, 35}\definecolor{second}{RGB}{74, 144, 226}\definecolor{third}{RGB}{189, 16, 224}\left[ \begin{array}{cccc}K_{out1} & K_{out2} & K_{out3}\\\end{array} \right] \times\left[ \begin{array}{cccc}\textcolor{first}{W_{k1o1}} & \textcolor{first}{W_{k1o2}} \\\textcolor{second}{W_{k2o1}} & \textcolor{second}{W_{k2o2}} \\\textcolor{third}{W_{k3o1}} & \textcolor{third}{W_{k3o2}} \\ \end{array} \right] +\left[ \begin{array}{cccc}b_{o1} & b_{o2} \\\end{array} \right]$$$$\downarrow$$$$\definecolor{first}{RGB}{245, 166, 35}\definecolor{second}{RGB}{74, 144, 226}\definecolor{third}{RGB}{189, 16, 224}\left[ \begin{array}{cccc}\textcolor{first}{W_{k1o1}} \times K_{out1} +\textcolor{second}{W_{k2o1}} \times K_{out2} +\textcolor{third}{W_{k3o1}} \times K_{out3} + b_{o1}\\\textcolor{first}{W_{k1o2}} \times K_{out1} +\textcolor{second}{W_{k2o2}} \times K_{out2} +\textcolor{third}{W_{k3o2}} \times K_{out3} + b_{o2}\\\end{array} \right]^{T} =\left[ \begin{array}{cccc}o_{in1} \\ o_{in2}\\\end{array} \right]^{T}$$<p style="">  Output으로 나가는 값(o<sub>out</sub>)은 우리가 예측하고자하는 Target 값을 가장 잘 보여줄 수 있는 형태로 만들어야한다.</p><p style="">  일반적으로는 <strong>회귀분석</strong>에는 특별한 <strong>활성화 함수 없이</strong> 내보내는 경우가 있지만, 만약 Target 데이터가 0보다 크고 1보다 작은 실수값만을 가진 상황이라면 Logistic Sigmoid을 사용했을 때 더 좋은 결과가 나올수도 있다는 의미다.</p><p style="">  이 포스트에서는 분류 문제라고 가정하여 활성화 함수로는 <strong>Softmax</strong>를 이용하여 확률값처럼 변환시키는 것을 예로 들겠다.</p></div><div class="clearfix" style="clear:both"><div style="float:left; width:40%; margin-right: 10px;">  <img src="/images/danial/back-prop/softmax_o.png"></div><div style="">$$Softmax = \mathrm{e}^{o_{ina}}/(\sum_{a=1}^{2}\mathrm{e}^{o_{ina}})$$$$Softmax(o_{in}) =\left[ \begin{array}{cccc}\mathrm{e}^{o_{in1}}/(\sum_{a=1}^{2}\mathrm{e}^{o_{ina}})   \\ \mathrm{e}^{o_{in2}}/(\sum_{a=1}^{2}\mathrm{e}^{o_{ina}})   \\\end{array} \right]^{T} = \left[ \begin{array}{cccc}o_{out1} \\ o_{out2}\\\end{array} \right]^{T}$$<p style="">이렇게 Output으로 나온 o<sub>out</sub> 벡터가 우리의 예측값 y_pred이다.물론 Random하게 초기화된 W, b값에 의해 예측한 값이라고 하기엔 터무니없는 값들이 나올 것임을 명심하자.</p></div></div><hr><h3 id="conclusion" href="#conclusion" style="color: #cb4a44; clear:both; margin-top: 10px;">정리</h3><img src="/images/danial/back-prop/network_detail.png"><p>여기까지 계산 과정은 자세히 알아보았다. 현실 데이터들을 사용하게 되면 저것보다 Layer 개수나 각 Layer의 Neuron 개수는 차이가 나겠지만, 행렬 곱이란 점에서 식을 쓰는 법은 사실상 차이가 없을 것이다.<br><br>Neural network에 대해서 다시 생각해보자면 실제 인간 뇌의 <strong>"Neuron들이 input이 들어오면 <strong>어떤 임계값(Threshold)</strong>이상의 전기신호가 존재해야 전달하는 것"</strong>에서 아이디어를 얻은 것이므로, <br> 위의 그림에서 J, K layer는 그런 Threshold가 넘는지를 <strong>활성화 함수(Activation function)</strong>을 통과시키며 해당 Neuron을 활성화할지 비활성화할지를 결정하는 과정인 것이다.<br><br>이제 고민해보자. input과 output은 우리가 측정한 실재하는 값이므로 변하지 않는 상수이지만, Hidden layer 내에서는 한 층을 통과할 때마다 새로운 값을 배출하고, 그 값을 다시 다음 층에서 받아서 또 통과시키면서 최초 input이 어떤 패턴을 가졌는지를 확인하는 과정이라고 볼 수 있지 않을까?<br><br>물론 패턴이라는 것은 해당 layer에서 <strong>활성화(우린 Sigmoid를 사용했으니, 0.5 이상인 경우를 활성화된 상태라고 하자)</strong>시킨 뉴런이 어떻게 분포해있는가를 말하는 것이고, 그 패턴을 해석한 파워는 input -> layer까지 오는 길에 계산한 W, b에서 나오는 것이므로, 이 값들을 우리는 더 <strong>정확한 패턴 해석</strong>을 위해서 학습하게 되는 것이다.<br><br>W, b가 우리의 학습하려는 값이라고 설명하면 끝나는 것을 길게 얘기했는데, 사실 필자도 이것이 어떻게 도움이 될지는 정확히 모르겠다. 하지만, 어떤 수학 문제가 있을 때 가장 쉽게 푸는 법을 알아내는 사람이 더 뛰어난 것이듯이,<br><br>분류 혹은 예측 성공률이 매우 높으면서도, 가장 적은 컴퓨팅 파워, 즉 해당 문제를 풀기 위한 가장 최적의 Layer 개수와 Neuron 개수가 존재할 수 있다는 것을 위의 해석 방식에서는 나타내고 있다는 점에서 의미가 있다고 본다.<br><br><blockquote>이 분야를 공부하는 사람이라면 3Blue1Brown이라는 유튜버를 알고 있을 텐데, 그가 <a href="https://www.youtube.com/watch?v=aircAruvnKk" target="_blank" rel="noopener">신경망이란 무엇인가? | 1장.딥러닝에 관하여</a>에서 설명한 것에서 영감을 얻어 말로 풀어 설명한 것이니 영상을 보며 정리하면 더 도움이 될 것.</blockquote></p><hr><h3 id="cost-function" href="#cost-function" style="clear:both; margin-top: 10px;">오차 함수(Cost function)</h3><p>로지스틱 활성 함수를 이용한 분류 문제를 풀 때는 정답 y가 클래스 k에 속하는 데이터에 대해서 k번째 값만 1이고 나머지는 0인 one-hot-encoding 벡터를 사용한다.</p><p>Cost-function은 Cross-Entropy Error를 사용한다.</p><script type="math/tex; mode=display">cross entropy = - (1/n)(\sum_{i=1}^{3} (y_{i} \times \log(o_{outi})) + ((1-y_{i}) \times \log((1-o_{outi}))))</script><h3 id="optimization" href="#optimization" style="clear:both; margin-top: 10px;">가중치 최적화</h3><p>오차함수를 최소화하기 위해 아래처럼 미분(gradient)을 사용한 <strong>Steepest gradient descent</strong> 방법을 적용하자. 여기서 &mu;는 step size 혹은 learning rate라고 부른다.</p><script type="math/tex; mode=display">w_{k+1} = w_{k} - \mu \frac{\partial C}{\partial w}</script><script type="math/tex; mode=display">b_{k+1} = b_{k} - \mu \frac{\partial C}{\partial b}</script><p>문제점은 단순하게 수치적으로 미분을 계산하게되면 모든 가중치에 대해서 개별적으로 미분을 계산해야하는 문제가 있다. 하지만 <strong>역전파 (Back propagation)</strong> 을 사용하면 모든 가중치에 대한 미분값을 한번에 계산할 수 있다.</p><p>다음 포스트에서 자세하게 알아보도록 하자!</p><blockquote><p>이번 <strong><neural network=""> 인공신경망에 대한 이해</neural></strong> 포스팅들은 시간을 많이 들여서 가능한 쉽고 직관적으로 설명하기 위해 노력하였다. 만약 도움이 되었다면! 공유를 부탁드린다!</p></blockquote><h3 style="clear:both; margin-top: 20px;"> Related Posts</h3><p><a href="https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;fbclid=IwAR2lsWOByt_MrzBkv5-Dc9P6JIdvHv1pUELE5q-0SVqQ73b6tS-RYGUI9eM" target="_blank" rel="noopener">Backpropagation calculus | Deep learning, chapter 4 by 3Blue1Brown</a><br><a href="https://datascienceschool.net/view-notebook/0178802a219c4e6bb9b820b49bf57f91/" target="_blank" rel="noopener">신경망 기초 이론</a></p>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Artificial Intelligence" scheme="https://databuzz-team.github.io/tags/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://databuzz-team.github.io/tags/Deep-Learning/"/>
    
      <category term="Neural Network" scheme="https://databuzz-team.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Deep Learning&gt; An introduction to Deep Learning with Tensorflow(Part-3)</title>
    <link href="https://databuzz-team.github.io/2018/10/30/Basic-deep-learning-tensorflow-for-beginner-3/"/>
    <id>https://databuzz-team.github.io/2018/10/30/Basic-deep-learning-tensorflow-for-beginner-3/</id>
    <published>2018-10-30T05:30:36.000Z</published>
    <updated>2018-11-01T14:45:15.239Z</updated>
    
    <content type="html"><![CDATA[<h3>About</h3><p>이번 포스트에서는 간단하게 <strong>회귀분석(Linear Regression)</strong> 의 개념과 <strong>Tensorflow</strong> 를 이용하여 학습하는 법에 대해서 알아보자.</p><h4 id="목차"><a href="#목차" class="headerlink" title="목차"></a>목차</h4><ul><li><a href="#linear-regression">회귀분석(Regression Analysis)이란?</a></li><li><a href="#tensorflow-regression">Tensorflow를 이용한 회귀 분석 실습</a><ul><li><a href="#preprocessing">데이터 전처리(Data preprocessing)</a></li><li><a href="#tensorflow-regerssion-modeling">Tensorflow Modeling</a></li></ul></li><li><a href="#jupter-notebook">Jupyter Notebook</a></li></ul><h3 id="linear-regression" href="#linear-regression">회귀분석(Regression Analysis)이란?</h3><p>위키피디아의 정의에 의하면,</p><blockquote><p>통계학에서, 선형 회귀(linear regression)는 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다.</p></blockquote><p>여기서 <strong>독립 변수</strong> 는 입력값이나 원인을 나타내며, <strong>종속 변수</strong> 는 결과물이나 효과를 나타낸다.</p><p>예) 집값을 예측하는 모델이라면 집값이 종속 변수(y)이고, 집의 위치, 방의 개수 등의 특징(Feature)들이 독립 변수(x)가 된다.</p><div><img src="https://upload.wikimedia.org/wikipedia/commons/b/be/Normdist_regression.png"><p style="width: 100%; text-align:center;"><a href="https://ko.wikipedia.org/wiki/%ED%9A%8C%EA%B7%80_%EB%B6%84%EC%84%9D" target="_blank" rel="noopener">출처 : 위키백과</a></p></div><p>위 그림은 독립 변수 1개와 종속 변수 1개를 가진 회귀 분석의 예이며, 그 중에서도 <strong>선형 회귀(Linear Regression)</strong> 를 한 예이다.</p><p>여기서 <strong>선형 회귀</strong> 에 대해서만 간단하게 소개하자면 가장 널리 사용되는 기법이며, 종속 변수와 독립 변수의 관계가 <strong>선형(Linear)</strong> 인 경우에 사용한다.</p><blockquote><p>회귀 분석 기법은 선형 회귀 외에도 다양한 종류가 있는데, 이 포스트에서 소개하지는 않을 예정이고, 나중에 <a href="https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/" target="_blank" rel="noopener">링크</a>의 내용을 번역하여 공유하도록 하겠다.(자세한 내용이 궁금한 사람은 위의 링크를 통해서 확인하자)</p></blockquote><h3 id="tensorflow-regression" href="#tensorflow-regression">Tensorflow를 이용한 회귀 분석 실습</h3><h4 id="preprocessing" href="#preprocessing">1. 데이터 전처리(Data preprocessing)</h4><blockquote><p>데이터 전처리는 이번 포스트의 주목적이 아니므로 자세한 설명을 더하진 않겠다.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import Dependencies</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 독립 변수와 종속 변수를 분리한다.</span></span><br><span class="line">X_data = pd.DataFrame(boston.data, columns=boston.feature_names)</span><br><span class="line">y_data = pd.DataFrame(boston.target, columns=[<span class="string">"Target"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Test 데이터를 분리한다</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># StandardScaler를 사용하여 스케일링한다</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 객체로 사용해야 나중에 Test데이터에 같은 Mean, Variance를 사용할 수 있다.</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(X_train)</span><br><span class="line">X_train = pd.DataFrame(data=scaler.transform(X_train), columns=X_train.columns, index=X_train.index)</span><br><span class="line"><span class="comment"># 주의 : Test 데이터는 fit을하면 안된다! 객체 내부에 설정된 Mean, Variance값이 Update되기 때문에 여기서는 transform만 사용한다.</span></span><br><span class="line">X_test = pd.DataFrame(data=scaler.transform(X_test), columns=X_test.columns, index=X_test.index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensorflow에서 사용할 땐 Numpy 데이터 타입으로 사용할 예정이니 변환하자</span></span><br><span class="line">X_train = np.array(X_train)</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line">X_test = np.array(X_test)</span><br><span class="line">y_test = np.array(y_test)</span><br><span class="line">type(X_train), type(y_train), type(X_test), type(y_test)</span><br></pre></td></tr></table></figure><p>이 예제에서는 Scaler로 <strong>StandardScaler(평균은 0으로, 표준편차는 1로 만드는 Scaler)</strong> 를 사용하였지만, 데이터마다 적절한 Scaler는 다를 수 있음을 명심하자. 역시 이 포스트에서는 자세히 다루지 않겠으며, 관심이 있는 사람들은 아래에 링크를 통해서 간략하게 이해를 하는 것을 추천한다.</p><ul><li><a href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" target="_blank" rel="noopener">Compare the effect of different scalers on data with outliers(Scikit-learn documentation)</a></li><li><a href="https://datascienceschool.net/view-notebook/f43be7d6515b48c0beb909826993c856/" target="_blank" rel="noopener">Scikit-Learn의 전처리 기능(데이터 사이언스 스쿨)</a></li></ul><p></p><h4 id="tensorflow-regerssion-modeling" href="#tensorflow-regerssion-modeling">2. Tensorflow Modeling</h4><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Learning Rate</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 가중치를 몇번 업데이트 할 것인가?</span></span><br><span class="line">epochs = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Features 독립 변수</span></span><br><span class="line">X = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, X_train.shape[<span class="number">1</span>]])</span><br><span class="line"><span class="comment"># Labels 종속 변수</span></span><br><span class="line">y = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight 가중치, 초기값은 정규분포에서 랜덤하게 뽑는다</span></span><br><span class="line">W = tf.Variable(tf.random_normal([X_train.shape[<span class="number">1</span>], <span class="number">1</span>]))</span><br><span class="line"><span class="comment"># Bias 초기값은 정규분포에서 랜덤하게 뽑는다</span></span><br><span class="line">b = tf.Variable(tf.random_normal([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p></p><p>여기서 <strong>placeholder()</strong> , <strong>Variable()</strong> 메서드 사용법에 대해서는 <a href="https://databuzz-team.github.io/2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/">이전 포스트</a>를 참고하자.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.Variable을 사용했거나, 메서드 내부적으로 변수가 존재하는 경우에는 Variables</span></span><br><span class="line"><span class="comment"># 초기화해줘야 한다.</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 우리가 예측하는 값 W*X + b</span></span><br><span class="line">hypothesis = tf.add(tf.matmul(X, W), b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cost function으로는 MSE를 사용</span></span><br><span class="line">cost = tf.reduce_mean(tf.square(y - hypothesis))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient Descent 방법으로 최적화</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cost_history를 기록하면 마지막에 epoch 변화에 따른 cost 변화를 확인할 때 편리하다</span></span><br><span class="line">cost_history = np.empty(shape=[<span class="number">1</span>], dtype=float)</span><br></pre></td></tr></table></figure><p>이렇게 필요한 Graph는 다 만들었으니, 이제 Session을 열어서 W, b를 update하며 Cost Function의 값을 최소화하는 작업을 실행하자.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">0</span>, epochs):</span><br><span class="line">        <span class="comment"># optimizer에서 반환하는 값은 의미가 없으니 _로 받아주자</span></span><br><span class="line">        _, err = sess.run([optimizer, cost], feed_dict=&#123;X: X_train, y: y_train&#125;)</span><br><span class="line"></span><br><span class="line">        cost_history = np.append(cost_history, err)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 100 번에 한번씩 Error 변화를 확인하자</span></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch: &#123;0&#125;, Error: &#123;1&#125;'</span>.format(epoch, err))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch: &#123;0&#125;, Error: &#123;1&#125;'</span>.format(epoch + <span class="number">1</span>, err))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 우리가 설정한 Epochs만큼의 학습이 끝난 후에 나온 값을 확인하기 위해 받아두자</span></span><br><span class="line">    updated_W = sess.run(W)</span><br><span class="line">    updated_b = sess.run(b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test 데이터를 예측한 값</span></span><br><span class="line">    y_pred = sess.run(hypothesis, feed_dict=&#123;X: X_test&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mean Squared Error</span></span><br><span class="line">    mse = sess.run(tf.reduce_mean(tf.square(y_pred - y_test)))</span><br></pre></td></tr></table></figure></p><p>위의 코드들을 통해서 <strong>Tensorflow Session</strong> 을 이용하여 <strong>회귀 분석(Regression Analysis)</strong> 방법을 알아보았고, 아래에 <strong>Jupyter notebook</strong> 에서는 <strong>Tensorflow</strong> 이 제공하는 <strong>Estimator API</strong> 를 사용하여 <strong>Linear Regression</strong> 하는 방법도 추가되어 있으니 도움이 되길 바란다.</p><h3 id="jupter-notebook" href="#jupter-notebook">Jupyter Notebook</h3><div class="notebook-embedded"><iframe src="https://nbviewer.jupyter.org/gist/DanialDaeHyunNam/6d96c11ac99bc2f2413ca5c8c6490dbc" width="100%" height="100%" frameborder="0" allowfullscreen></iframe></div><p>이 다음 포스트에서는 <strong>Tensorflow</strong> 로 <strong>인공 신경망(Neural Network)</strong> 을 구현하는 법에 대해서 작성하겠다.</p><h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3><ul><li><a href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" target="_blank" rel="noopener">Compare the effect of different scalers on data with outliers(Scikit-learn documentation)</a></li><li><a href="https://datascienceschool.net/view-notebook/f43be7d6515b48c0beb909826993c856/" target="_blank" rel="noopener">Scikit-Learn의 전처리 기능(데이터 사이언스 스쿨)</a></li></ul>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Data Science" scheme="https://databuzz-team.github.io/tags/Data-Science/"/>
    
      <category term="Machine Learning" scheme="https://databuzz-team.github.io/tags/Machine-Learning/"/>
    
      <category term="Tensorflow" scheme="https://databuzz-team.github.io/tags/Tensorflow/"/>
    
      <category term="Regression" scheme="https://databuzz-team.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>&lt;MACHINE LEARNING&gt;Catboost 알아보기</title>
    <link href="https://databuzz-team.github.io/2018/10/24/Catboost/"/>
    <id>https://databuzz-team.github.io/2018/10/24/Catboost/</id>
    <published>2018-10-24T06:47:02.000Z</published>
    <updated>2018-11-05T08:51:13.621Z</updated>
    
    <content type="html"><![CDATA[<p>Machine Learning을 공부하였다면 한번쯤 XGBoost와 LightGBM, H2O를 들어보았을 것이다. 최근 이 분야에서 기존의 기술들을 위협하는 새로운 기술이 나와 이를 소개하고자 한다.</p><p>이 글은 <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">참고블로그</a> (Towards Data Science)와 <a href="https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/" target="_blank" rel="noopener">참고기사</a>의 내용을 번역 및 요약 정리하였고 좀 더 자세한 내용은 링크를 통해 확인해볼 수 있다.</p><h3 id="Catboost란-무엇인가"><a href="#Catboost란-무엇인가" class="headerlink" title="Catboost란 무엇인가?"></a>Catboost란 무엇인가?</h3><p>Catboost란 Yandex에서 개발된 오픈 소스 Machine Learning이다. 이 기술은 다양한 데이터 형태를 활용하여 기업이 직면한 문제들을 해결하는데 도움을 준다. 특히 분류 정확성에서 높은 점수를 제공한다.</p><p>Catboost는 Category와 Boosting을 합쳐서 만들어진 이름이다.<br>여기에서 Boost는 Gradient boosting machine learnin algorithm에서 온 말인데 Gradient boosting은 추천 시스템, 예측 등 다양한 분야에서 활용되어지는 강력한 방법이고 Deep Learning과 달리 적은 데이터로도 좋은 결과를 얻을 수 있는 효율적인 방법이다.</p><h3 id="왜-Catboost를-활용하는가"><a href="#왜-Catboost를-활용하는가" class="headerlink" title="왜 Catboost를 활용하는가?"></a>왜 Catboost를 활용하는가?</h3><h4 id="더-좋은-결과"><a href="#더-좋은-결과" class="headerlink" title="더 좋은 결과"></a>더 좋은 결과</h4><p>Catboost는 Benchmark에서 더 좋은 결과를 얻었다.</p><div>    <img src="https://cdn-images-1.medium.com/max/1600/1*vsg1IUlGtzCoNuGo9XqGwg.png">    <span style="font-size:11px; text-align:center; display:block; color: #999;">        <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">            GBDT Algorithms Benchmark          </a>    </span></div><h4 id="Category-features-사용의-편리성"><a href="#Category-features-사용의-편리성" class="headerlink" title="Category features 사용의 편리성"></a>Category features 사용의 편리성</h4><p>Category features를 사용하기 위해서는 One-Hot-Encoding등 데이터를 전처리할 필요가 있었지만 Catboost에서는 사용자가 다른 작업을 하지 않아도 자동으로 이를 변환하여 사용한다. 이 분야를 공부한 경험이 있다면 이 기능이 얼마나 편리한지를 알 수 있을 것이다. 자세한 내용은 <a href="https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/" target="_blank" rel="noopener">document</a>를 통해 확인할 수 있다.</p><h4 id="빠른-예측"><a href="#빠른-예측" class="headerlink" title="빠른 예측"></a>빠른 예측</h4><p>학습 시간이 다른 GBDT에 보다는 더 오래 걸리는 대신에 예측 시간이 13-16배 정도 더 빠르다.</p><div>    <img src="https://cdn-images-1.medium.com/max/2000/1*BE8PZe54DMWe6gFdHlYsxg.png">    <span style="font-size:11px; text-align:center; display:block; color: #999;">        <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">            Left : CPU, Right : GPU        </a>    </span></div><h4 id="더-나은-기능들"><a href="#더-나은-기능들" class="headerlink" title="더 나은 기능들"></a>더 나은 기능들</h4><ul><li>default parameters값으로 더 나은 성능<br>hyper-parmeter tuning을 하지 않더라도 기본적인 세팅으로도 좋은 결과를 얻을 수 있어 활용성이 뛰어나다. 자세한 내용은 <a href="https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning-docpage/" target="_blank" rel="noopener">document</a>를 통해 확인할 수 있다.</li></ul><div>    <img src="https://cdn-images-1.medium.com/max/1600/1*znsWIb1X3Eez5LjNf4mg_g.png">    <span style="font-size:11px; text-align:center; display:block; color: #999;">        <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">            GBDT Algorithms with default parameters Benchmark        </a>    </span></div><ul><li>feature interactions</li></ul><div>    <img src="https://cdn-images-1.medium.com/max/1600/1*VV1eH5Iwz3hJmKWAaV_Y6w.png">    <span style="font-size:11px; text-align:center; display:block; color: #999;">        <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">    Catboost’s Feature Interactions     </a>    </span></div><ul><li>feature importances</li></ul><div>    <img src="https://cdn-images-1.medium.com/max/1600/1*6Y9gHBQLxk-PoIJLd2wr1g.png">    <span style="font-size:11px; text-align:center; display:block; color: #999;">        <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">    Catboost’s Feature Importance     </a>    </span></div><ul><li>object(row) importances</li></ul><div>    <img src="https://cdn-images-1.medium.com/max/1600/1*ZoMzKdiIyLU9wDelELQMvg.png">    <span style="font-size:11px; text-align:center; display:block; color: #999;">        <a href="https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2" target="_blank" rel="noopener">    Catboost’s Object Importance    </a>    </span></div><ul><li>the snapshot</li></ul><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><p>프로젝트 등을 수행하다보면 Catgory feature를 이용하는 것이 상당히 번거롭다는 것을 알 수 있을 것이다. 뿐만 아니라 예측 시간이 오래걸린다면 실제로 시스템에 적용하는데는 큰 문제점을 가지고 있음을 알고 있다.</p><p>다른 Maching Learning algorithms의 단점을 보완해주는 Catboost를 잘 활용한다면 좀 더 나은 시스템을 개발하는데 도움이 될 것이다.</p><h3 id="유용한-자료"><a href="#유용한-자료" class="headerlink" title="유용한 자료"></a>유용한 자료</h3><ul><li><a href="https://tech.yandex.com/catboost/doc/dg/concepts/about-docpage/" target="_blank" rel="noopener">Catboost Documentation</a></li><li><a href="https://github.com/catboost/catboost" target="_blank" rel="noopener">Catboost Github</a></li><li><a href="https://catboost.ai/" target="_blank" rel="noopener">Catboost official website</a></li><li><a href="https://arxiv.org/abs/1706.09516" target="_blank" rel="noopener">CatBoost: unbiased boosting with categorical features</a></li></ul>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="HyunGeun Yoon" scheme="https://databuzz-team.github.io/categories/HyunGeun-Yoon/"/>
    
    
      <category term="Machine Learning" scheme="https://databuzz-team.github.io/tags/Machine-Learning/"/>
    
      <category term="Catboost" scheme="https://databuzz-team.github.io/tags/Catboost/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Deep Learning&gt; An introduction to Deep Learning with Tensorflow(Part-2)</title>
    <link href="https://databuzz-team.github.io/2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/"/>
    <id>https://databuzz-team.github.io/2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/</id>
    <published>2018-10-24T01:03:01.000Z</published>
    <updated>2018-10-29T00:26:14.549Z</updated>
    
    <content type="html"><![CDATA[<p>이번 포스트에서는 Tensorflow에서 꼭 알아야 할 기본적인 지식들에 대해서 알아보자.(Tensorflow설치법에 대해서는 다루지 않을 것)</p><h3 id="1-Dataflow"><a href="#1-Dataflow" class="headerlink" title="1. Dataflow"></a>1. Dataflow</h3><div style="width:40%; float:left;"><img src="https://www.tensorflow.org/images/tensors_flowing.gif"><p style="text-align:center"><a href="https://www.tensorflow.org/guide/graphs" target="_blank" rel="noopener">출처 :Graphs and Sessions</a></p></div><div style="width: 60%; float:right; color: #333;"><a href="https://databuzz-team.github.io/2018/10/22/Basic-deep-learning-tensorflow-for-beginner/">이전 포스트 Part 1</a>에서 이미 다룬 내용인데다 개념 설명이라 지루할 수도 있지만, Tensorflow를 사용하기 위해선 꼭 이해해야 하는 부분을 그림과 함께 짧게 설명하고자 하니, 이 부분을 꼭 읽어주시길 바란다.옆의 그림은 <strong>Tensorflow</strong> 공식 홈페이지에 나와있는데, 정말 직관적으로 Tensorflow의 <strong>Dataflow Graph(Node And Operation)</strong> 를 표현해냈다.먼저 <strong>Tensor</strong> 는 옆에서 보이는 검은 라인이고(Edge), <strong>Operation</strong> 은 노드(Node)들, 그림에서 타원들을 의미한다. 즉, Tensor가 Operation으로 들어가서 해당 Operation에서 설정한 연산을 진행하고 다시 Tensor를 Output으로 내보내는 것이다.<br><br><blockquote>필자가 이해한 바대로라면 Tensor나 Operation이라는 낯선 단어들을 사용해서 어렵게 느껴지지만 결국은 함수의 기능을 한다고 봐주면 되겠다. 차이점은 Graph는 선언이고 Session을 통해서 Run을 한다는 것</blockquote><br>물론! 끝은 Output으로 값을 내보내는 것을 목적으로 하는 것은 아니다. 우리의 목적은 <strong>W 가중치</strong> 를 <strong>Update</strong> 하는 것이므로, 마지막에 우리가 <strong>Optimizer</strong> 의 변수로 설정한 <strong>W1, b1, W2, b2 ...</strong> 들이 <strong>Update</strong> 되는 것으로 <strong>Session.run()</strong> 이 종료된다.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(task)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></div><div style="width: 100%; clear:both; color: #333;"><h3>2. Operation의 name과 scope의 간략한 소개</h3>본격적으로 코드에 대해 설명하기 전에 <strong>Debugging</strong> 에 도움이 되는 정보인 Operation name에 대해서 간략하게 살펴만 보자.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "c"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미 사용된 이름은 자동으로 유니크화 시킨다.</span></span><br><span class="line">c_1 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scope는 접두사로 붙게되는데 나중에 설명할 Tensorboard에서 확인할 때 훨씬 편리하다.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scope 아래로는 경로로 계층을 표현한다.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">  c_4 = tf.constant(<span class="number">4</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/c_1"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_5 = tf.constant(<span class="number">5</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation 이름은 "outer/inner_1/c"</span></span><br></pre></td></tr></table></figure><p style="text-align:center; margin:0"><a href="https://www.tensorflow.org/guide/graphs" target="_blank" rel="noopener">출처 :Graphs and Sessions</a></p></div><div style="width: 100%; clear:both; color: #333;"><h3>3. Tensorflow 가장 많이 사용되는 함수들을 알아보자</h3>이 포스트에서는 필자가 많이 사용된다고 생각하는 가장 기본적인 함수들만 작성했는데, 이 외에도 거의 모든 수학 연산은 다 구현되어 있으니 자세한 API는 <a href="https://www.tensorflow.org/api_docs/python/" target="_blank" rel="noopener">링크</a>를 통해서 찾아보도록 하자.<ul><li><h5>tf.placeholder()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><strong>tf.placeholder()</strong> 는 머신러닝에서 무조건 사용하는 함수이며, 구조는 위의 코드에서 보듯 dtype, shape, name으로 이루어져있는데 핵심은 <strong>shape</strong> 이다.<br><br><blockquote>다른 포스트에서 언급하겠지만 tensorflow에서 shape을 이해하는 것은 매우 중요하다.</blockquote><br>예를 들어 집값을 예측하는 모델을 우리가 만들고 있고, 집의 <strong>Feature(특징)</strong> 는 <strong>rooms, is_riverside</strong> 로 이루어져 있다고 하자. 그리고 만약 우리가 최종적으로 사용할 <strong>Feature</strong> 의 수는 위에서 말한 두 가지면 충분하다고 결정했다고 본다면, <strong>Input</strong> 데이터의 <strong>shape</strong> 으로 <strong>[?, 2]</strong>, 즉 컬럼은 2 개로 결정을 한 상태라는 것이다. 하지만, 집의 개수, 즉 데이터의 개수는 많을수록 좋은 것이므로 변동의 여지가 언제나 있는 값일 뿐 아니라 최종적으로 우리가 새로운 데이터를 예측하려 할 때에도 변하는 값이라는 것이다.<br><br>그런 이유에서 <strong>tensorflow</strong> 에서는 <strong>placeholder</strong> 라는 함수를 제공하는 것이며, <strong>Feature(X)</strong> 와 <strong>Label(y)</strong> 은 <strong>placeholder</strong> 를 사용해서 넣어준다. 주의할 점은 <strong>sess.run()</strong> 시에 <strong>feed_dict</strong> 에 꼭 값을 직접 <strong>넣어(feed)</strong> 주어야 한다는 것.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x_data = np.array([[<span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>]])</span><br><span class="line">y_data = np.array([[<span class="number">120000</span>], [<span class="number">100000</span>], [<span class="number">200000</span>]])</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">2</span>], name=<span class="string">"X"</span>)</span><br><span class="line"><span class="comment"># 이 예제에서는 각 집마다의 가격을 예측하는 것이므로, shape은 [None, 1]이 된 것.</span></span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"y"</span>)</span><br></pre></td></tr></table></figure></li><li><h5>tf.Variable()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</span><br></pre></td></tr></table></figure>머신러닝을 통해서 구하고자하는 값인 <strong>Weight</strong> 나 <strong>Bias</strong> 와 같은 값은 <strong>tensorflow</strong> 의 <strong>tf.Variable()</strong> 함수를 사용해서 선언해야한다. 구조는 위의 code에서 보듯 아주 단순하며, 보통은 Random하게 초기화하는 경우가 많으므로 행렬곱을 할 상대인 <strong>X</strong> 와 예측 값으로 내보내는 <strong>y</strong> 의 <strong>shape</strong> 을 고려해서 <strong>tf.random_normal()</strong> 을 사용하게 된다.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>]), name=<span class="string">'wight'</span>)</span><br><span class="line">b = tf.Variable(tf.random_normal([<span class="number">1</span>]), name=<span class="string">'bias'</span>)</span><br></pre></td></tr></table></figure></li><li><h5>tf.matmul()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.matmul(</span><br><span class="line">    a,</span><br><span class="line">    b,</span><br><span class="line">    transpose_a=<span class="keyword">False</span>,</span><br><span class="line">    transpose_b=<span class="keyword">False</span>,</span><br><span class="line">    adjoint_a=<span class="keyword">False</span>,</span><br><span class="line">    adjoint_b=<span class="keyword">False</span>,</span><br><span class="line">    a_is_sparse=<span class="keyword">False</span>,</span><br><span class="line">    b_is_sparse=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>머신러닝에서는 <strong>원소간의 곱(Element-wise multiplication)</strong> 보다는 <strong>행렬곱(Matrix multiplication)</strong> 이 훨씬 많이 쓰이므로 <strong>tf.matmul()</strong> 은 꼭 알아야하는 함수이다.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hypothesis = tf.matmul(X, W) + b</span><br></pre></td></tr></table></figure></li><li><h5>tf.train module</h5>오늘 소개할 마지막은 함수가 아닌 <strong>모듈(Module)</strong> 이다. 아래는 가장 보편적인 <strong>Optimizer</strong> 인 <strong>GradientDescentOptimizer</strong> 로 예를 들었지만, 훨씬 많은 모델들을 <strong>tensorflow</strong> 에서는 제공하고 있으니, 이 외에 필요한 정보는 <a href="https://www.tensorflow.org/api_docs/python/tf/train" target="_blank" rel="noopener">링크</a>에서 확인하도록 하자.<br><br><blockquote><strong>목표 함수(Cost function)</strong> 에 대해서 이 포스트에서는 특별히 다루지 않지만, 다음 포스트들에서 CNN, RNN 등의 알고리즘을 구현하며 설명을 추가하겠다.</blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cost = tf.reduce_mean(tf.square(hypothesis - y))</span><br><span class="line">train = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)</span><br></pre></td></tr></table></figure></li></ul><h3>4. 마무리</h3>이번 포스트는 어려운 내용이 없지만 <strong>tensorflow</strong> 를 공부하며 간단한 모델들을 구현해보며 가장 자주 사용되고 중요하다고 느꼈던 점을 정리해보았는데, 처음 시작하는 사람들에게 꼭 도움이 되길 바란다.<br><br>아래는 가장 간단하게 회귀분석 모델을 구현한 코드이며, 위에서 설명한 개념들을 정말 간단한 예제이긴 하지만, 대략적으로 어떻게 쓰이나 보여주기 위해서 작성해보았다.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x_data = np.array([[<span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>]])</span><br><span class="line">y_data = np.array([[<span class="number">120000</span>], [<span class="number">100000</span>], [<span class="number">200000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper parameter</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">n_epoch = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">2</span>], name=<span class="string">"X"</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>]), name=<span class="string">'wight'</span>)</span><br><span class="line">b = tf.Variable(tf.random_normal([<span class="number">1</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line">hypothesis = tf.matmul(X, W) + b</span><br><span class="line"></span><br><span class="line">cost = tf.reduce_mean(tf.square(hypothesis - y))</span><br><span class="line">train = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 변수가 있는 경우에는 초기화를 실행해줘야 한다.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># train이 반환하는 값은 우리에게 필요없다.</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(n_epoch):</span><br><span class="line">        c, _ = sess.run([cost, train], feed_dict=&#123;X: x_data, y: y_data&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Step :"</span>, step, <span class="string">"Cost :"</span>, c)</span><br><span class="line">            <span class="comment"># x, y를 임의로 만든거라..</span></span><br><span class="line">            <span class="comment"># 이 부분은 train data를 학습시키는지 확인하는 목적 외에는 없다.</span></span><br><span class="line">            print(sess.run(hypothesis, feed_dict=&#123;X: x_data&#125;))</span><br></pre></td></tr></table></figure></div><h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Machine Learning" scheme="https://databuzz-team.github.io/tags/Machine-Learning/"/>
    
      <category term="Artificial Intelligence" scheme="https://databuzz-team.github.io/tags/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://databuzz-team.github.io/tags/Deep-Learning/"/>
    
      <category term="Tensorflow" scheme="https://databuzz-team.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Deep Learning&gt; An introduction to Deep Learning with Tensorflow(Part-1)</title>
    <link href="https://databuzz-team.github.io/2018/10/22/Basic-deep-learning-tensorflow-for-beginner/"/>
    <id>https://databuzz-team.github.io/2018/10/22/Basic-deep-learning-tensorflow-for-beginner/</id>
    <published>2018-10-22T14:27:01.000Z</published>
    <updated>2018-10-28T13:47:10.819Z</updated>
    
    <content type="html"><![CDATA[<p><br></p><p>최근에 번역한 <a href="https://databuzz-team.github.io/2018/10/15/The-Most-in-Demand-Skills-for-Data-Scientists/">“DATA SCIENTISTS에게 가장 요구되는 기술(SKILLS)들”</a> 글에서 확인했듯, 급부상하는 분야인 Machine Learning에서도 Deep Learning은 우리가 앞으로 이 분야에서 일하자면 꼭 공부해야 한다.</p><p>먼저 간단하게 <strong>Machine Learning</strong> 에 대해서 알아보자.</p><h3 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning?"></a>Machine Learning?</h3><p>사실 역사는 이미 오래된 분야다. 하지만 최근들어서야 급부상하는 이유는 무엇보다도 Computing Power의 성장과 방대한 데이터가 있다는 점이다.</p><p>크게는 지도학습과 비지도학습으로 나뉘는데, 지도학습은 Input과 Output을 알려주고 그 사이에 존재하는 로직을 기계가 학습하도록 하는 것이며, 비지도학습은 Input만 알려주고 데이터가 가진 특징 속에서 스스로 학습해나가는 것을 말한다.</p><p>아래의 영상은 최근에 공개된 영상인데 Deep Learning의 현재 수준이 이미 놀라운 수준임을 알 수 있다.</p><iframe src="https://www.youtube.com/embed/PCBTZh41Ris" width="560" height="315" frameborder="0" allowfullscreen></iframe><p>만약 가장 트렌디한 머신러닝 기술들을 보고싶다면 <a href="https://arxiv.org/" target="_blank" rel="noopener">링크</a>를 확인하는 것이 가장 좋다고한다. 학회에 제출하고 논문이 Accept가 되는데 평균적으로 7개월이 걸리는데, 그 기간이면 이미 새로운 기술이 나오는 상황이라.. 만약 트렌드한 기술을 캐치하자면 꼭 <a href="https://arxiv.org/" target="_blank" rel="noopener">링크</a>에서 확인하자.</p><h3 id="왜-Tensorflow인가"><a href="#왜-Tensorflow인가" class="headerlink" title="왜 Tensorflow인가?"></a>왜 Tensorflow인가?</h3><p>Tensorflow는 Google이 개발한 Library인데, 현재는 가장 사랑을 받고 있는 Library로써, Google 자체에서도 Google Photo, Google Voice Search 등은 모두 Tensorflow를 간접적으로 사용하고 있는 App 들이다.</p><h3 id="Tensorflow-기본-개념"><a href="#Tensorflow-기본-개념" class="headerlink" title="Tensorflow 기본 개념"></a>Tensorflow 기본 개념</h3><blockquote><p>이 부분은 <a href="https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278" target="_blank" rel="noopener">A beginner introduction to TensorFlow (Part-1)</a>을 번역 정리하였다.</p></blockquote><ul><li><strong>Tensorflow</strong> 의 Core는 그래프(computational graph)와 Tensor로 이루어져있다.</li><li><strong>Tensor와 Vector의 차이점</strong> 은 Tensor는 크기만 가진경우도 존재한다는 점이다. 즉 Vector는 Tensor의 특수상황이며, 부분집합으로 볼 수 있다.</li><li><p><strong>Tensor</strong> 는 이해했으니 <strong>Flow</strong> 를 살펴보자. <strong>Flow</strong> 는 computational graph 혹은 단순한 graph라고 볼 수 있다. cyclic한 구조는 아니며, 각 노드(아래 그림에서 동그라미)는 덧셈, 뺄셈 등의 기능을 가지고 있다. <div><img src="https://cdn-images-1.medium.com/max/1600/1*7lklTJQytHz8w7Eeqz5ZhA.png"><span style="font-size:12px; text-align:center; display:block; color: #999;"><a href="https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278" target="_blank" rel="noopener">출처 : A beginner introduction to TensorFlow (Part-1)</a></span></div></p></li><li><p><strong>e = (a+b) * (b+1)</strong><br>기능(operation)적인 역할을 해야 하는 모든 연결되는 노드(꼭짓점)은 graph의 시작일 수는 없고, Tensor를 받거나 새로운 Tensor를 생성하는 역할을 한다. 또한, computational graph는 항상 복잡한 계층 구조로 되어있는데, 위의 그림에서도 마찬가지로 표현되어 있듯 a+b는 c로 b+1은 d로 표현될 수 있다.</p></li><li><p><strong>e = (c)*(d), c = a+b and b = b+1</strong><br>위의 그림에서 명백하게 표현되어있듯 각 노드는 전 노드에 의존적이어서 c는 a, b 없이 나올 수 없고, e는 c, d 없이 나올 수 없다. 단 c, d처럼 같은 계층에 존재하는 노드들은 상호 독립적이다. 이 점은 computational graph를 이해할 때 가장 중요한 부분으로써, <strong>같은 레벨에 있는 노드 c의 경우는 d가 먼저 계산되어야 할 이유가 없고, 평행적으로 실행될 수 있다.</strong></p></li><li><p>위에서 설명한 computational graph의 평행 관계(parallelism)가 가장 중요한 개념이니 꼭 숙지해야 한다. 이 평행 관계의 의미는 c 계산이 끝나지 않았다고, d 계산은 평행적으로 이루어진다는 점이며, tensorflow는 이 부분을 멋지게 해낸다.</p></li></ul><h3 id="Tensorflow-분할-실행"><a href="#Tensorflow-분할-실행" class="headerlink" title="Tensorflow 분할 실행"></a>Tensorflow 분할 실행</h3><div><img src="https://cdn-images-1.medium.com/max/1600/1*cok4bMhTvE93UdGmRblEyw.png"><span style="font-size:12px; text-align:center; display:block; color: #999;"><a href="https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278" target="_blank" rel="noopener">출처 : A beginner introduction to TensorFlow (Part-1)</a></span></div><ul><li><strong>Tensorflow</strong> 는 여러 기계에 평행적으로 계산을 실행하여 훨씬 빠른 연산을 할 수가 있는데, 따로 설정할 필요 없이 내부적으로 설정이 된다.<br><br><blockquote><p>위의 그림에서 왼쪽은 single Tensorflow session을 사용한 경우라서 single worker가 존재하는 것이고, 오른쪽은 multiple workers를 사용한 경우</p></blockquote></li></ul><ul><li>Worker들은 서로 다른 기기에서 독립적으로 연산을 하고 다음 노드에 해당하는 Worker에게 Result를 넘겨준다. 이때, Delay로 인한 성능 저하가 일어날 수 있는데 이는 주로 <strong>Tensor</strong> 의 <strong>Size</strong> 에서 발생하므로 어떤 <strong>자료형</strong> 을 설정할 것인지가 <strong>중요한 문제</strong> 다.</li></ul><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><p>이번 Part1 글에서는 Machine Learning과 Tensorflow에 대한 소개를 중점으로 썼고, Part2에서는 Tensorflow의 기본 문법과 MNIST Digit 이미지 분류하는 코딩에 대해서 포스팅하겠다.</p><hr><h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3><p><a href="https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278" target="_blank" rel="noopener">A beginner introduction to TensorFlow (Part-1)</a></p>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Machine Learning" scheme="https://databuzz-team.github.io/tags/Machine-Learning/"/>
    
      <category term="Artificial Intelligence" scheme="https://databuzz-team.github.io/tags/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://databuzz-team.github.io/tags/Deep-Learning/"/>
    
      <category term="Tensorflow" scheme="https://databuzz-team.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Github&gt; Gist를 사용하여 Jupyter Notebook 포스팅하기</title>
    <link href="https://databuzz-team.github.io/2018/10/21/Github-Gist/"/>
    <id>https://databuzz-team.github.io/2018/10/21/Github-Gist/</id>
    <published>2018-10-21T13:01:32.000Z</published>
    <updated>2018-11-01T14:43:01.347Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Gist를-사용하면-아래처럼-소스코드를-임베딩-할-수-있다"><a href="#Gist를-사용하면-아래처럼-소스코드를-임베딩-할-수-있다" class="headerlink" title="Gist를 사용하면 아래처럼 소스코드를 임베딩 할 수 있다."></a><a href="https://gist.github.com/" target="_blank" rel="noopener">Gist</a>를 사용하면 아래처럼 소스코드를 임베딩 할 수 있다.</h5><p>아래의 예는 <strong>Jupyter Notebook</strong> 을 임베딩한 것이지만, 이 외에도 .py, .md, .html 등 소스 코드는 다 할 수 있다.</p><script src="https://gist.github.com/DanialDaeHyunNam/afc48b2814cd7798ee7dbaa00e321468.js"></script><h5 style="color: red;">  만약 위의 박스 안에 Jupyter Notebook이 보이지 않는다면.. Gist 사이트에 올린 .ipynb 파일이 Rendering 실패하는 것이므로 <a href="#new-method">여기</a>를 클릭해서 불필요한 내용은 Skip 하시길..</h5><p>사용하는 방법은 아래와 같다.</p><hr><h3 id="1-먼저-Gist-사이트로-이동"><a href="#1-먼저-Gist-사이트로-이동" class="headerlink" title="1. 먼저 Gist 사이트로 이동"></a>1. 먼저 <a href="https://gist.github.com/" target="_blank" rel="noopener">Gist</a> 사이트로 이동</h3><p><img src="/images/danial/gist_1.png"><br>위의 사진처럼 코드를 적는 창이 있다. Jupyter Notebook의 경우에는 해당 파일을 <strong>Drag &amp; Drop</strong> 하면 된다.</p><h3 id="2-Indent-Mode를-Spaces에서-Tabs로-변경"><a href="#2-Indent-Mode를-Spaces에서-Tabs로-변경" class="headerlink" title="2. Indent Mode를 Spaces에서 Tabs로 변경"></a>2. Indent Mode를 Spaces에서 Tabs로 변경</h3><p><img src="/images/danial/gist_2.png"><br>이 경우는 <strong>Jupyter Notebook</strong> 을 임베딩하는 경우에만 적용될 수도 있는 사항이니 조심하자.</p><p>필자는 Jupyter Notebook을 처음부터 도전하였는데, Spaces의 경우에는 계속 Render를 실패하기에 이유를 찾아보다가 발견한 방법이다.</p><h3 id="3-Script-Copy"><a href="#3-Script-Copy" class="headerlink" title="3. Script Copy"></a>3. Script Copy</h3><p><img src="/images/danial/gist_3.png"><br>이미지처럼 Embed 옆에 아이콘을 클릭하면 아래처럼 <strong>script</strong> 태그가 복사된다. Markdown 파일이나 HTML에 붙여넣기하면 된다.<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://gist.github.com/&#123;UserId&#125;/&#123;script&#125;.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h3 id="Tip-1-이후에-다른-소스코드를-포스팅하고자-하면-Add-file이-아닌-New-gist로-새로운-파일들을-추가하면-된다"><a href="#Tip-1-이후에-다른-소스코드를-포스팅하고자-하면-Add-file이-아닌-New-gist로-새로운-파일들을-추가하면-된다" class="headerlink" title="Tip 1) 이후에 다른 소스코드를 포스팅하고자 하면 Add file이 아닌 New gist로 새로운 파일들을 추가하면 된다."></a>Tip 1) 이후에 다른 소스코드를 포스팅하고자 하면 Add file이 아닌 New gist로 새로운 파일들을 추가하면 된다.</h3><p><img src="/images/danial/gist_helper.png"></p><h3 id="Tip-2-이미-Gist에서-정해준-iframe의-크기를-조절하고-싶은-경우에는-아래의-css를-추가해주면-된다"><a href="#Tip-2-이미-Gist에서-정해준-iframe의-크기를-조절하고-싶은-경우에는-아래의-css를-추가해주면-된다" class="headerlink" title="Tip 2) 이미 Gist에서 정해준 iframe의 크기를 조절하고 싶은 경우에는 아래의 css를 추가해주면 된다."></a>Tip 2) 이미 Gist에서 정해준 iframe의 크기를 조절하고 싶은 경우에는 아래의 css를 추가해주면 된다.</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.gist</span>&#123;</span><br><span class="line">  <span class="attribute">max-width</span>: <span class="number">80%</span>;</span><br><span class="line">  <span class="attribute">margin-top</span>: <span class="number">10px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.gist-data</span>&#123;</span><br><span class="line">  <span class="attribute">max-height</span>: <span class="number">300px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.gist</span> <span class="selector-tag">iframe</span><span class="selector-class">.render-viewer</span>&#123;</span><br><span class="line">  <span class="attribute">max-height</span>: <span class="number">260px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 style="color: blue;">  Gist가 Rendering을 문제없이 해냈다면 여기서부터 아래 내용은 볼 필요가 없다.</h5><hr><h3 id="new-method" href="#new-method">  New) Gist에서 제공하는 방법보다는 불편하고 깔끔하지는 않지만, Jupyter Notebook을 Embed하는 다른 방법이 있어 소개하고자 한다.</h3><blockquote><p>이 방법의 경우에는 사실 github repo에 올린 jupyter notebook에도 적용되는 방법이므로, 꼭 Gist를 사용할 필요는 없지만 이 포스트에서는 Gist에 올린 경우에 대해서만 설명하겠다.</p></blockquote><h3 id="1-Gist-사이트에-업로드"><a href="#1-Gist-사이트에-업로드" class="headerlink" title="1. Gist 사이트에 업로드"></a>1. Gist 사이트에 업로드</h3><p>일단 위의 순서 중에 <a id="1-먼저-Gist-사이트로-이동" href="#1-먼저-Gist-사이트로-이동">1.Gist 사이트로 이동</a>는 필요하므로 Drag&amp;Drop 하는 부분까지는 진행을 하자.</p><h3 id="2-업로드한-Gist의-주소를-복사"><a href="#2-업로드한-Gist의-주소를-복사" class="headerlink" title="2. 업로드한 Gist의 주소를 복사"></a>2. 업로드한 Gist의 주소를 복사</h3><p>1번에서 업로드한 Gist의 URL을 복사한다.<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://gist.github.com/&#123;id&#125;/&#123;key-value&#125;</span><br></pre></td></tr></table></figure></p><h3 id="3-https-nbviewer-jupyter-org-로-이동"><a href="#3-https-nbviewer-jupyter-org-로-이동" class="headerlink" title="3. https://nbviewer.jupyter.org/ 로 이동"></a>3. <a href="https://nbviewer.jupyter.org/" target="_blank" rel="noopener">https://nbviewer.jupyter.org/</a> 로 이동</h3><p><a href="https://nbviewer.jupyter.org/" target="_blank" rel="noopener">nbviewer 사이트</a>로 이동을 하면,<br><strong>URL | GitHub username | GitHub username/repo | Gist ID</strong> 를 입력하는 Input 창이 있다. 거기에 2번에서 복사한 주소를 넣으면 <strong>해당 Notebook이 Rendering</strong> 된 화면으로 넘어간다.</p><h3 id="4-Hexo-Tag-Plugins를-이용해서-Embedding하면-끝"><a href="#4-Hexo-Tag-Plugins를-이용해서-Embedding하면-끝" class="headerlink" title="4. Hexo Tag Plugins를 이용해서 Embedding하면 끝!"></a>4. Hexo Tag Plugins를 이용해서 Embedding하면 끝!</h3><p>필자는 아래의 Markdown에 html을 같이 사용했다.<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">'notebook-embedded'</span>&gt;</span></span></span><br><span class="line">&#123;% iframe https://nbviewer.jupyter.org/gist/&#123;id&#125;/&#123;unknown-value&#125; 100% 100% %&#125;</span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p><p>여기서 굳이 <a href="https://hexo.io/docs/tag-plugins.html" target="_blank" rel="noopener">Hexo Tag Plugins</a>에서 제공한<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% iframe [width] [height] %&#125;</span><br></pre></td></tr></table></figure></p><p>방식의 표현식을 사용한 것은 Markdown preview에서 iframe이 계속해서 reload되는 것이 싫어서 그랬을 뿐 html 태그를 사용해도 무방하다.</p><p>div에 class를 준 것은 원하는 디자인 틀을 만들기 위해서였고 css는 아래와 같다.<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.notebook-embedded</span>&#123;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#eee</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">40px</span> solid <span class="number">#eee</span>;</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">4px</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">400px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="5-적용한-결과는-아래와-같다"><a href="#5-적용한-결과는-아래와-같다" class="headerlink" title="5. 적용한 결과는 아래와 같다"></a>5. 적용한 결과는 아래와 같다</h3><div class="notebook-embedded"><iframe src="https://nbviewer.jupyter.org/gist/DanialDaeHyunNam/afc48b2814cd7798ee7dbaa00e321468" width="100%" height="100%" frameborder="0" allowfullscreen></iframe></div>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Git" scheme="https://databuzz-team.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Machine Learning&gt; 비대칭 데이터 문제는 어떻게 해결해야 하나?</title>
    <link href="https://databuzz-team.github.io/2018/10/21/Handle-Imbalanced-Data/"/>
    <id>https://databuzz-team.github.io/2018/10/21/Handle-Imbalanced-Data/</id>
    <published>2018-10-21T02:23:13.000Z</published>
    <updated>2018-10-23T07:34:46.842Z</updated>
    
    <content type="html"><![CDATA[<p><br></p><blockquote><p><a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="noopener">참고 블로그(8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset) 링크</a><br><a href="https://machinelearningmastery.com/" target="_blank" rel="noopener">블로거 링크</a></p></blockquote><p>아직 데이터 사이언스에 입문한지 오래되지는 않았지만, 개인 프로젝트로 인스타그램 데이터를 크롤링하여 작업하면서 <strong>비대칭 데이터 문제(Imbalanced Data)</strong> 에 부딪혔다.</p><p>생각해보면 현실세계에서 우리가 예측하고자 하는 클래스가 Uniform하게 분포되어있을 확률은 낮은 것이 당연하다.</p><h3 id="비대칭-데이터란"><a href="#비대칭-데이터란" class="headerlink" title="비대칭 데이터란?"></a>비대칭 데이터란?</h3><p>비대칭 데이터는 일반적으로 분류 문제에서 클래스들이 균일하게 분포하지 않은 문제를 의미한다.</p><p>간단한 예를 들자면, 100개의 과일 사진 중에 사과 사진이 90개, 귤 사진이 10개인 경우다.</p><p>이 경우라면 100개 중에 랜덤하게 뽑은 사진이 무슨 사진인지 맞춰야 한다면 사과라고 말하는 것이 가장 합리적일 것이다. 이것이 비대칭 데이터에서 일어나는 가장 큰 문제점이다.</p><hr><h3 id="비대칭-데이터-문제를-다루는-8가지-전략"><a href="#비대칭-데이터-문제를-다루는-8가지-전략" class="headerlink" title="비대칭 데이터 문제를 다루는 8가지 전략"></a>비대칭 데이터 문제를 다루는 8가지 전략</h3><p>이제 비대칭 문제에 대해서는 이해했을 것이다. 이제 문제를 해결하기 위해 취해야 할 전략(서두에서 링크한 블로그에서 소개한 8가지 전략)을 알아보자.</p><h3 id="1-데이터를-더-모을-수-있나"><a href="#1-데이터를-더-모을-수-있나" class="headerlink" title="1. 데이터를 더 모을 수 있나?"></a>1. 데이터를 더 모을 수 있나?</h3><p>너무 당연한 질문! 더 많은 데이터는 당연히 조금은 더 클래스 대칭적인 결과를 제공할 것이므로..</p><h3 id="2-평가-기준을-바꿔보자"><a href="#2-평가-기준을-바꿔보자" class="headerlink" title="2. 평가 기준을 바꿔보자."></a>2. 평가 기준을 바꿔보자.</h3><p><strong>Accuracy</strong> 는 비대칭 문제에서는 사용하면 안되는 평가 기준이다(<a href="https://en.wikipedia.org/wiki/Accuracy_paradox" target="_blank" rel="noopener">Accuracy Paradox</a>를 참고하자).</p><p>원문 저자는 자신의 포스트 <a href="https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/" target="_blank" rel="noopener">Classification Accuracy is Not Enough: More Performance Measures You Can Use</a> 에서 소개한 평가기준들을 추천했다.</p><ul><li>Confusion Matrix:<br>예측 결과를 테이블 형태로 보여준다.</li><li>Precision:<br>Positive 클래스에 속한다고 출력한 샘플 중 실제로 Positive 클래스에 속하는 샘플 수의 비율</li><li>Recall<br>실제 Positive 클래스에 속한 샘플 중에 Positive 클래스에 속한다고 출력한 표본의 수</li><li>F1 Score<br>정밀도(Precision)과 재현율(Recall)의 가중 조화 평균</li></ul><p>이 외에도</p><ul><li>Kappa(or Cohen’s kappa)<br>Accuracy를 정규화한 값으로 보여준다.</li><li>ROC Curves<br>ROC(Receiver Operator Characteristic) 커브는 클래스 판별 기준값의 변화에 따른 위양성률(fall-out)과 재현율(recall)의 변화를 시각화한 것이다.</li></ul><h3 id="3-데이터셋을-Re-샘플링하자"><a href="#3-데이터셋을-Re-샘플링하자" class="headerlink" title="3. 데이터셋을 Re-샘플링하자."></a>3. 데이터셋을 Re-샘플링하자.</h3><p>데이터 셋을 변형시켜서 전체 클래스의 분포를 균일하게 만드는 방법으로 두 가지 방법이 있다.</p><ol><li>Over-sampling</li><li>Under-sampling</li></ol><p>둘 다 사용해보는 것을 추천한다.</p><p><a href="https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/" target="_blank" rel="noopener">비대칭 데이터 문제</a> &lt;- 이 링크에서 다양한 샘플링 방법을 시각화해서 설명한 자료들이 있으니 확인하자.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 다양한 샘플링 방법을 구현한 파이썬 패키지이다.</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$pip</span> install -U imbalanced-learn</span><br></pre></td></tr></table></figure><h3 id="4-가짜-데이터-샘플을-만들자"><a href="#4-가짜-데이터-샘플을-만들자" class="headerlink" title="4. 가짜 데이터 샘플을 만들자."></a>4. 가짜 데이터 샘플을 만들자.</h3><p>Over sampling의 기법은 가짜 데이터를 더 생성하는 것이니 위의 방법을 좀 더 발전시킨 전략이라고 보면 되겠다.</p><p><strong>Naive Bayes</strong> 알고리즘을 사용할 경우에는 생성도 가능하니 이를 이용하거나, 가장 인기있는 방법인 <strong>SMOTE(Synthetic Minority Over-sampling Technique)</strong> 를 사용하는 것을 추천한다.</p><p>SMOTE는 부족한 클래스의 모조 샘플을 만들어내는 것이다. 이 알고리즘은 2개 이상의 비슷한 객체들을 선택하여 거리를 재고 사이사이 새로운 데이터를 생성해나간다.</p><p>자세한 정보는 <a href="http://www.jair.org/papers/paper953.html" target="_blank" rel="noopener">링크</a>를 확인하자.</p><h3 id="5-다른-Algorithms을-사용해보자"><a href="#5-다른-Algorithms을-사용해보자" class="headerlink" title="5. 다른 Algorithms을 사용해보자."></a>5. 다른 Algorithms을 사용해보자.</h3><p>언제나 그렇듯, 자신이 가장 좋아하는 알고리즘을 모든 문제에 사용하지 않는 것을 추천한다.</p><p><strong>의사 결정 나무(Decision Tree)</strong> 는 비대칭 문제에서 성능이 좋은 경우가 많다.</p><p>C4.5, C5.0, CART and Random Forest 등 다양하게 사용해보는 것을 추천.</p><h3 id="6-모델에-제한을-준다"><a href="#6-모델에-제한을-준다" class="headerlink" title="6. 모델에 제한을 준다."></a>6. 모델에 제한을 준다.</h3><p><strong>Penalized classification(패널티가 있는 분류)</strong> 는 함수를 설정하여 부족한 클래스를 분류하는 것에 오류가 일어나게 만드는 것을 의미한다. 제한사항으로 설정한 함수(패널티 함수)는 부족한 클래스를 분류하는 것에 좀 더 집중을 할 수 있게 한다.</p><blockquote><p>penalized-SVM, penalized-LDA 등 penalized 된 버젼들이 존재한다.<br>그뿐만아니라, 패널라이즈드 모델들을 위해 Framework도 존재하는데, 예를들어 Weka의 <a href="http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html#CostSensitiveClassifier--" target="_blank" rel="noopener">CostSensitiveClassifier</a>가 있다.</p></blockquote><p>패널티 매트릭스를 만드는 것은 매우 복잡하여, 특정 알고리즘을 써야 하거나 Re-샘플링이 불가능한 경우에 사용하는 것이 좋다.</p><h3 id="7-다른-관점으로-시도하자"><a href="#7-다른-관점으로-시도하자" class="headerlink" title="7. 다른 관점으로 시도하자."></a>7. 다른 관점으로 시도하자.</h3><p>추천하는 방법으로는 <strong>Anomaly Detection</strong>, <strong>Change Detection</strong> 가 있다.</p><ul><li><a href="https://en.wikipedia.org/wiki/Anomaly_detection" target="_blank" rel="noopener">Anomaly Detection</a></li><li><a href="https://en.wikipedia.org/wiki/Change_detection" target="_blank" rel="noopener">Change Detection</a></li></ul><hr><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><p>위의 방법들은 당연하게 시행되어야 하는 순서를 보여준다고 생각한다. 조금 허무할 수도 있지만, 결국은 데이터 <strong>사이언티스트(Scientist)</strong> 라는 단어가 의미하듯.. 실험적인 정신을 가지고 다양한 각도에서 도전하고 가장 좋은 결과를 내기 위해 최선을 다해야 한다는 것..</p><hr><h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3><p><a href="https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/" target="_blank" rel="noopener">분류 성능 평가(데이터 사이언스 스쿨)</a></p>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Machine Learning" scheme="https://databuzz-team.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Medium 블로그 번역&gt; Data Scientists에게 가장 요구되는 기술(Skills)들</title>
    <link href="https://databuzz-team.github.io/2018/10/15/The-Most-in-Demand-Skills-for-Data-Scientists/"/>
    <id>https://databuzz-team.github.io/2018/10/15/The-Most-in-Demand-Skills-for-Data-Scientists/</id>
    <published>2018-10-15T12:28:03.000Z</published>
    <updated>2018-10-29T05:53:46.477Z</updated>
    
    <content type="html"><![CDATA[<p>요즘 글로벌 시장에서 <strong>Data Scientists</strong> 부족으로 난리라고 한다.. 가장 요구되는 기술들이 무엇인지 분석한 기사가 있어서 번역을 통해서 소개하고자 한다 (자세한 내용은 아래의 링크를 통해서 원문을 확인하자).</p><blockquote><p><a href="https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db" target="_blank" rel="noopener">원문(The Most in Demand Skills for Data Scientists) 링크</a><br><a href="https://towardsdatascience.com/@jeffhale" target="_blank" rel="noopener">저자 Jeff Hale 블로그 링크</a></p></blockquote><p>기사에서는 조사지역은 미국, 키워드는 “Data Scientist”와 “다른 키워드”를 AND 타입으로 검색하여 데이터 사이언티스트와 연관 키워드가 완전히 매칭되는 결과들만을 모아서 조사하였다. 사용한 채용 사이트는 <strong>LinkedIn, Indeed, SimplyHired, Monster, AngelList</strong>로 5가지이다.</p><p>먼저 아래의 그래프를 통해 요구되는 일반적 스킬들에 대해서 알아보자.<br>(그래프를 만드는데 사용된 자료 및 과정은 <a href="https://www.kaggle.com/discdiver/the-most-in-demand-skills-for-data-scientists/" target="_blank" rel="noopener">Jeff Hale Kaggle Kernel</a>을 참고할 것).</p><div><img src="https://cdn-images-1.medium.com/max/800/1*-oG0j_wGSW_9cNNs4_qgFQ.png"><span style="font-size:12px; text-align:center; display:block; color: #999;"><a href="https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db" target="_blank" rel="noopener">출처 : The Most In Demand Skills For Data Scientists</a></span></div><p>원본 데이터를 통해서 확인해보면 LinkedIn을 제외한 다른 채용사이트들에서는 Analysis의 비중이 더 높았다. 그래서 전체를 Combine한 결과를 보여주는 위의 그래프에서는 Analysis가 가장 비중이 높은 것으로 보이는 것이다.</p><p>원문 저자가 얻은 Insight를 요약하자면,</p><ul><li>Machine Learning은 예측 시스템을 만드는 가장 핵심 기술이므로 가장 요구되는 기술.</li><li>Data Science는 통계 및 Computer Science기술이 필수.</li><li>Communication이 절반에 가까운 잡 포스팅에서 요구되고 있는 점.</li><li>AI, Deep Learning의 빈도수가 높지 않았으나, 이는 Machine Learning의 부분집합이므로 생긴 현상이다.</li><li>Deep Learning의 경우는 점점 더 사용되는 경우가 늘고 있다. 예를 들어 자연어 처리의 경우에도 가장 많이 사용되고 있는 알고리즘은 Deep Learning. <strong>미래에 Deep Learning 스킬이 훨씬 많은 곳에서 요구하게 될 것이며, Machine Learning은 즉 Deep Learning을 의미하게 될 것이라 본다.</strong></li></ul><p>다음으로는 회사가 원하는 <strong>Technology Skills</strong> 에 대해서 살펴보자.</p><div><img src="https://cdn-images-1.medium.com/max/800/1*jnZT4gFAzScOJ_VnYsni0g.png"><span style="font-size:12px; text-align:center; display:block; color: #999;"><a href="https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db" target="_blank" rel="noopener">출처 : The Most In Demand Skills For Data Scientists</a></span></div><ol><li><strong>Python</strong><br>1위는 Python. 말이 필요 없다.</li><li><strong>R</strong><br>R은 Python과 비중에서 큰 격차를 보이진 않는데, 이는 R이 Python 이전에 Data Science의 필수 언어였기 때문이다. R의 최초 목적은 통계학을 위해서 시작되었고, 아직도 많은 통계학자들에게 사랑을 받는다.</li><li><strong>SQL</strong><br>SQL은 간혹 Data Science 세계에서 무시되는 경우도 있지만, 실제로는 많은 회사에서 요구되며 해당 스킬을 가지고 있으면 채용시장에서 당연히 훨씬 유리하다.</li><li><strong>Hadoop And Spark</strong><br>둘은 모두 Apache에서 제공되는 빅데이터를 위한 Open Source Tool이다.<br>이 두 가지 스킬은 Medium에서나 Tutorial 등이 눈에 띄게 적게 다뤄지고 있다. 지원자(취준) 중에도 앞서 설명한 기술들을 사용하지만 이 두 가지는 못하는 경우가 훨씬 많으므로, Hadoop과 Spark를 할 줄 안다면 채용시장에서 확실히 유리해질 것이다.</li><li><strong>Java and SAS</strong><br>저자는 이 두 가지 스킬이 이렇게 요구되는 사실에 놀랐다. 이 둘은 보통 Data Science 커뮤니티에서 그렇게 주목받고 있지는 않다.</li><li><strong>Tableau</strong><br>Tableau는 분석 플랫폼 및 시각화 도구이며, 사용법이 간편하다. 현재 무료 버젼이 있지만, 데이터를 Private하게 보관하려면 유료이다.</li></ol><p>저자는 만약 <strong>Tableau</strong> 을 사용한 경험이 없다면, Udemy의 <a href="https://www.udemy.com/tableau10/" target="_blank" rel="noopener">Tableau 강의</a> 를 추천했다.</p><p>아래 그래프는 2017년과 2018년을 비교한 것이다.</p><div><img src="https://cdn-images-1.medium.com/max/800/1*iueZKOOBidZtr-FTYyf6QA.png"><span style="font-size:12px; text-align:center; display:block; color: #999;"><a href="https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db" target="_blank" rel="noopener">출처 : The Most In Demand Skills For Data Scientists</a></span></div><p>전체적으로 비슷하다. 하지만, <strong>R, Hadoop, Java, SAS and Matlab</strong> 의 요구도는 내려가고 있고, <strong>Tableau</strong> 는 확실히 올라가고 있다는 점은 주목할만하다.</p><h4 id="결론-추천-사항들"><a href="#결론-추천-사항들" class="headerlink" title="결론(추천 사항들)"></a>결론(추천 사항들)</h4><ul><li>데이터 분석은 기본, Machine Learning 기술을 높이는데 집중하자.</li><li>Communication 능력에 투자할 것. 추천 책으로는 <a href="https://www.amazon.com/Made-Stick-Ideas-Survive-Others/dp/1400064287" target="_blank" rel="noopener">Made to Stick</a>이 있고, <a href="http://www.hemingwayapp.com" target="_blank" rel="noopener">Hemmingway Editor</a>을 통해서 writing 능력도 키우자.</li><li>Deep Learning 프레임워크들을 마스터하자. Deep Learning 프레임워크을 능숙하게 사용할 수 있어야한다. 저자가 쓴 다른 <a href="https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a" target="_blank" rel="noopener">블로그 글</a>도 참고할 것.</li><li>만약 R과 Python 중에 배울 언어를 고민 중이라면, Python을 선택할 것. 먼저 Python이 능숙하다면 R을 배우는 것을 고려해볼 것(채용시장에서 더 유리할 것이다).</li></ul><p>회사에서 Python을 사용하는 Data Scientist을 구한다는 것은 당연히 Data Science Libraries(<strong>numpy, pandas, scikit-learn, matplotlib 등</strong>)를 다루길 기대하는 것이다.</p><p>만약 Deep learning을 배우고자 한다면, <strong>Keras, FastAI</strong> 를 <strong>TensorFlow or PyTorch</strong> 이전에 시작할 것을 추천한다. Keras 공부를 위한 추천 책은 <a href="https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438" target="_blank" rel="noopener">Deep Learning with Python</a>이다.</p><p>끝으로 채용사이트는 <strong>LinkedIn</strong>을 추천한다.</p>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Data Science" scheme="https://databuzz-team.github.io/tags/Data-Science/"/>
    
      <category term="Tech News" scheme="https://databuzz-team.github.io/tags/Tech-News/"/>
    
  </entry>
  
  <entry>
    <title>&lt;MACHINE LEARNING&gt; CHAPTER1. 한눈에 보는 머신 러닝</title>
    <link href="https://databuzz-team.github.io/2018/10/13/hands-on-chapter1/"/>
    <id>https://databuzz-team.github.io/2018/10/13/hands-on-chapter1/</id>
    <published>2018-10-13T05:34:56.000Z</published>
    <updated>2018-10-28T09:55:53.090Z</updated>
    
    <content type="html"><![CDATA[<p>Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow 책을 읽고 공부하면서 내용을 요약하고 정리한 것입니다.<br><a id="more"></a></p><h3 id="1-1-머신-러닝이란"><a href="#1-1-머신-러닝이란" class="headerlink" title="1.1 머신 러닝이란?"></a>1.1 머신 러닝이란?</h3><blockquote><p>어떤 작업 T에 대한 컴퓨터 프로그램의 성능을 P로 측정했을 때 경험E로 인해 성능이 향상됐다면, 이 컴퓨터 프로그램은 작업 T와 성능 측정 P에 대해 경험 E로 학습한 것이다.</p><pre><code>                                     - 톰 미첼(Tom Mitchell, 1997)</code></pre></blockquote><p>위키백과 문서를 모두 내려받는 것 -&gt; 많은 데이터를 갖게 되는 것 : 머신 러닝 X</p><h3 id="1-2-왜-머신-러닝을-사용하는가"><a href="#1-2-왜-머신-러닝을-사용하는가" class="headerlink" title="1.2 왜 머신 러닝을 사용하는가?"></a>1.2 왜 머신 러닝을 사용하는가?</h3><ul><li>기존 솔루션으로는 많은 수동 조정과 규칙이 필요한 문제 : 하나의 머신러닝 모델이 코드를 간단하고 더 잘 수행되도록 할 수 있습니다.</li><li>전통적인 방식으로는 전혀 해결 방법이 없는 복잡한 문제 : 가장 뛰어난 머신러닝 기법으로 해결 방법을 찾을 수 있습니다.</li><li>유동적인 환경 : 머신러닝 시스템은 새로운 데이터에 적응할 수 있습니다.</li><li>복잡한 문제와 대량의 데이터에서 통찰 얻기</li></ul><h3 id="1-3-머신러닝-시스템의-종류"><a href="#1-3-머신러닝-시스템의-종류" class="headerlink" title="1.3 머신러닝 시스템의 종류"></a>1.3 머신러닝 시스템의 종류</h3><ul><li>사람의 감독 하에 훈련하는 것인지 그렇지 않은 것인지(지도, 비지도, 준지도, 강화 학습)</li><li>실시간으로 점진적인 학습을 하는지 아닌지(온라인 학습과 배치 학습)</li><li>단순하게 알고 있는 데이터 포인트와 새 데이터 포인트를 비교하는 것인지 아니면 훈련 데이터셋에서 과학자들처럼 패턴을 발견하여 예측 모델을 만드는지(사례 기반 학습과 모델 기반 학습)</li></ul><p>-&gt; 서로 배타적이지 않으며 연결 가능</p><h4 id="1-3-1-지도-학습과-비지도-학습"><a href="#1-3-1-지도-학습과-비지도-학습" class="headerlink" title="1.3.1  지도 학습과 비지도 학습"></a>1.3.1  지도 학습과 비지도 학습</h4><p>학습하는 동안의 감독 형태나 정보량’에 따라 분류</p><h4 id="지도학습-supervised-learning"><a href="#지도학습-supervised-learning" class="headerlink" title="지도학습(supervised learning)"></a>지도학습(supervised learning)</h4><p>-&gt; 훈련데이터에 레이블 포함</p><ul><li>회귀(regression) : <strong>예측 변수</strong><sup>predictor variable</sup>라 부르는 <strong>특성</strong><sup>featrue</sup>(주행거리, 연식, 브랜드 등)을 사용해 중고차 가격 같은 <strong>타깃</strong> 수치를 예측하는 것</li><li>분류 (Classification) : 전형적 지도 학습</li></ul><p>가장 중요한 지도 학습 알고리즘</p><ul><li>k-최근접 이웃 <sup> k-Nearest Neighbors</sup></li><li>선형 회귀 <sup>Linear Regression</sup></li><li>로지스틱 회귀 <sup>Logistic Regression</sup></li><li>서포트 벡터 머신 <sup>Support Vector Machines(SVM)</sup></li><li>결정 트리 <sup>Decision Tree</sup>와 랜덤 포레스트 <sup>Random Forests</sup></li><li>신경망 <sup>Neural networks</sup></li></ul><h4 id="비지도-학습-unsupervised-learning"><a href="#비지도-학습-unsupervised-learning" class="headerlink" title="비지도 학습(unsupervised learning)"></a>비지도 학습(unsupervised learning)</h4><p>-&gt; 훈련데이터에 레이블 미포함</p><p>가장 중요한 비지도 학습 알고리즘</p><ul><li>군집 <sup>clustering</sup><ul><li>k-평균<sup>k-Means</sup></li><li>계층 군집 분석<sup>Hierarchical Cluster Analysis</sup>(HCA)</li><li>기댓값 최대화<sup>Expectation Maximization</sup></li></ul></li><li>시각화<sup>visualization</sup>와 차원 축소<sup>dimensionality reduction</sup><ul><li>주성분 분석<sup>Principal Component Analysis</sup>(PCA)</li><li>커널<sup>kernel</sup>PCA</li><li>지역적 선형 임베딩<sup>Locally-Linear Embedding</sup>(LLE)</li><li>t-SNE<sup>t-distributed Stochastic Neighbor Embedding</sup></li></ul></li><li>연관 규칙 학습<sup>Assiociation rule learning</sup><ul><li>어프라이어리<sup>Apriori</sup></li><li>이클렛<sup>Eclat</sup></li></ul></li></ul><p>시각화<sup>visualization</sup>알고리즘 : 도식화 가능한 2D나 3D 표현 , 가능한 구조 유지<br>차원 축소<sup>dimensionality reduction</sup> : 상관관계가 있는 여러 특성을 하나로 합치는 것  ex) 주행거리, 연식 -&gt; 차의 마모 (<strong>특성 추출</strong>)<br>이상치 탐지<sup>anomaly detection</sup> : 학습 알고리즘 주입 전 데이터셋에 이상한 값을 자동으로 제거<br>연관 규칙 학습<sup>association rule learning</sup> : 대량의 데이터에서 특성 간의 흥미로운 관계</p><h4 id="준지도-학습semisupervised-learning"><a href="#준지도-학습semisupervised-learning" class="headerlink" title="준지도 학습semisupervised learning"></a>준지도 학습<sup>semisupervised learning</sup></h4><p>레이블이 일부만 존재</p><h4 id="강화-학습Reinforcement-Learning"><a href="#강화-학습Reinforcement-Learning" class="headerlink" title="강화 학습Reinforcement Learning"></a>강화 학습<sup>Reinforcement Learning</sup></h4><p>학습하는 시스템을 <strong>에이전트</strong>, 환경을 관찰해서 행동을 실행하고 보상 또는 벌점을 받습니다. 가장 큰 보상을 얻기 위해 <strong>정책</strong><sup>policdy</sup>이라 부르는최상 전략을 스스로 학습합니다.</p><h4 id="1-3-2-배치-학습과-온라인-학습"><a href="#1-3-2-배치-학습과-온라인-학습" class="headerlink" title="1.3.2 배치 학습과 온라인 학습"></a>1.3.2 배치 학습과 온라인 학습</h4><p>입력 데이터의 스트림<sup>stream</sup>으로부터 점진적으로 학습할 수 있는 여부</p><p><strong>배치 학습</strong><sup>batch learning</sup></p><ul><li>가용한 데이터를 모두 사용해 훈련</li><li>제품 시스템에 적용하면 더 이상의 학습없이 실행</li><li>많은 컴퓨팅 자원 필요(CPU, 메모리 공간, 디스크 공간, 디스크 IO, 네트워크 IO 등)</li><li>자원이 제한된 시스템(예 - 스마트폰, 화성 탐사 로버)이 스스로 학습해야 할 때 많은 자원 사용하면 심각한 문제</li></ul><p><strong>온라인 학습</strong><sup>online learning</sup></p><ul><li>데이터를 순차적으로 한 개씩 또는 미니배치<sup>mini-batch</sup>라 부르는 작은 묶음 단위로 주입</li><li>빠른 변화에 스스로 적응해야하는 시스템에 적합, 컴퓨팅 자원이 제한된 경우</li><li>메인 메모리에 들어갈 수 없는 아주 큰 데이터셋을 학습하는 시스템(<strong>외부 메모리</strong><sup>out-of-core</sup> 학습)</li><li>전체 프로세스는 보통 오프라인, 따라서 <strong>점진적 학습</strong><sup>incremental learning</sup>으로 생각</li><li><strong>학습률</strong><sup>learning rate</sup> : 변화는 데이터에 얼마나 빠르게 적응할 것</li></ul><h4 id="1-3-3-사례-기반-학습과-모델-기반-학습"><a href="#1-3-3-사례-기반-학습과-모델-기반-학습" class="headerlink" title="1.3.3 사례 기반 학습과 모델 기반 학습"></a>1.3.3 사례 기반 학습과 모델 기반 학습</h4><p>어떻게 <strong>일반화</strong>되는가에 따라 분류</p><p><strong>사례 기반 학습</strong><sup>instance-based learning</sup></p><ul><li><strong>유사도</strong><sup>similarity</sup>를 측정하여 새로운 데이터를 일반화</li></ul><p><strong>모델 기반 학습</strong><sup>model-based learning</sup></p><ul><li>모델을 만들어 <strong>예측</strong>에 사용<ul><li>데이터를 분석</li><li>모델 선택</li><li>훈련 데이터로 모델 훈련(비용 함수<sup>cost function</sup> 최소화 하는 모델 파라미터 탐색)</li><li>새로운 데이터에 모델을 적용해 예측, 잘 일반화되길 기대</li></ul></li></ul><h3 id="1-4-머신러닝의-주요-도전-과제"><a href="#1-4-머신러닝의-주요-도전-과제" class="headerlink" title="1.4 머신러닝의 주요 도전 과제"></a>1.4 머신러닝의 주요 도전 과제</h3><p>문제점</p><ol><li>나쁜 알고리즘</li><li>나쁜 데이터</li></ol><h4 id="1-4-1-충분하지-않은-양의-훈련-데이터"><a href="#1-4-1-충분하지-않은-양의-훈련-데이터" class="headerlink" title="1.4.1 충분하지 않은 양의 훈련 데이터"></a>1.4.1 충분하지 않은 양의 훈련 데이터</h4><h4 id="1-4-2-대표성-없는-훈련-데이터"><a href="#1-4-2-대표성-없는-훈련-데이터" class="headerlink" title="1.4.2 대표성 없는 훈련 데이터"></a>1.4.2 대표성 없는 훈련 데이터</h4><ul><li>샘플이 작으면 <strong>샘플링 잡음</strong><sup>sampling noise</sup>(즉, 우연에 의한 대표성 없는 데이터)</li><li>샘플이 큰 경우도 추출 방법이 잘못된 경우 <strong>샘플링 편향</strong><sup>sampling bias</sup></li></ul><h4 id="1-4-3-낮은-품질의-데이터"><a href="#1-4-3-낮은-품질의-데이터" class="headerlink" title="1.4.3 낮은 품질의 데이터"></a>1.4.3 낮은 품질의 데이터</h4><ul><li>에러, 이상치<sup>outlier</sup>, 잡음</li><li>이상치가 명확하면 무시하거나 수동으로 잘못된 것을 고침</li><li>일부 특성 중 데이터가 누락된 경우 특성을 무시할지, 샘플을 무시할지, 빠진값을 채울지, 특성을 넣은 모델과 제외한 모델을 따로 훈련 시킬것인지 결정</li></ul><h4 id="1-4-4-관련-없는-특성"><a href="#1-4-4-관련-없는-특성" class="headerlink" title="1.4.4 관련 없는 특성"></a>1.4.4 관련 없는 특성</h4><ul><li><strong>특성 공학</strong><sup>feature engineering</sup> : 훈련에 사용할 좋은 특성들을 찾는 것<ul><li><strong>특성 선택</strong><sup>feature selection</sup> : 가지고 있는 특성 중에서 훈련에 가장 유용한 특성을 선택</li><li><strong>특성 추출</strong><sup>feature extraction</sup> : 특성을 결합하여 더 유용한 특성을 만듬(차원 축소 알고리즘)</li><li>새 특성을 만듬</li></ul></li></ul><h4 id="1-4-5-훈련-데이터-과대적합"><a href="#1-4-5-훈련-데이터-과대적합" class="headerlink" title="1.4.5 훈련 데이터 과대적합"></a>1.4.5 훈련 데이터 과대적합</h4><ul><li><strong>과대적합</strong><sup>overfitting</sup> : 모델이 훈련 데이터에 너무 잘 맞지만 일반성이 떨어짐<ul><li>훈련 데이터에 있는 잡음의 양에 비해 모델이 너무 복잡할 때 발생</li><li>파라미터 수가 적은 모델 선택, 훈련데이터에 특성수를 줄임, 모델에 제약을 가하여 단순화(<strong>하이퍼파라미터</strong><sup>hyperparameter</sup> : 학습하는 동안 적용할 규제의 양, 학습 알고리즘의 파라미터)</li><li>훈련 데이터를 더 많이 모음</li><li>훈련 데이터의 잡음을 줄임</li></ul></li></ul><h4 id="1-4-6-훈련-데이터-과소적합"><a href="#1-4-6-훈련-데이터-과소적합" class="headerlink" title="1.4.6 훈련 데이터 과소적합"></a>1.4.6 훈련 데이터 과소적합</h4><ul><li><strong>과소적합</strong><sup>underfitting</sup> : 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할 때<ul><li>파라미터가 더 많은 강력한 모델 선택</li><li>더 좋은 특성 제공(특성 엔지니어링)</li><li>모델의 제약을 줄임( 규제 하이퍼파라미터를 감소)</li></ul></li></ul><h3 id="1-5-테스트와-검증"><a href="#1-5-테스트와-검증" class="headerlink" title="1.5 테스트와 검증"></a>1.5 테스트와 검증</h3><ul><li><strong>훈련 세트</strong> 와 <strong>테스트 세트</strong> 로 나누어 훈련<ul><li><strong>일반화 오차</strong><sup>generalization error</sup>(<strong>외부 샘플 오차</strong><sup>out-of-sample erro</sup>) : 새로운 샘플에 대한 오류 비율</li><li>훈련 오차가 낮지만 일반화 오차가 높다면 과대 적합</li></ul></li><li><strong>검증 세트</strong><sup>validation set</sup><ul><li><strong>교차 검증</strong><sup>cross-validation</sup> 기법</li></ul></li></ul>]]></content>
    
    <!-- <summary type="html">
    
      &lt;p&gt;Hands-On Machine Learning with Scikit-Learn &amp;amp; TensorFlow 책을 읽고 공부하면서 내용을 요약하고 정리한 것입니다.&lt;br&gt;
    
    </summary> -->
    
      <category term="HyunGeun Yoon" scheme="https://databuzz-team.github.io/categories/HyunGeun-Yoon/"/>
    
    
      <category term="Machine Learning" scheme="https://databuzz-team.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>&lt;Hexo&gt; 블로그에 수식 사용하기 - mathjax 설정</title>
    <link href="https://databuzz-team.github.io/2018/10/13/hexo-mathjax/"/>
    <id>https://databuzz-team.github.io/2018/10/13/hexo-mathjax/</id>
    <published>2018-10-13T05:30:55.000Z</published>
    <updated>2018-10-28T09:55:53.091Z</updated>
    
    <content type="html"><![CDATA[<p>필자의 개인 블로그에 포스팅한 내용을 가져와 소개합니다. (<a href="https://hyeshinoh.github.io/2018/10/24/hexo_mathjax_00/" target="_blank" rel="noopener">원문 블로그</a>)</p><p>Hexo 블로그에서 LaTex로 수식을 작성할 수 있도록 mathjax를 설정하는 방법을 정리해보겠습니다.</p><h2 id="1-설치"><a href="#1-설치" class="headerlink" title="1. 설치"></a>1. 설치</h2><h3 id="1-renderer-설치-및-세팅"><a href="#1-renderer-설치-및-세팅" class="headerlink" title="1) renderer 설치 및 세팅"></a>1) renderer 설치 및 세팅</h3><p>Hexo의 기본 renderer인 hexo-renderer-marked는 mathjax 문법을 지원하지 않는다고 합니다. 따라서 다음과 같이 새로운 rendering engine으로 교체해줍니다.</p><p><code>$ npm uninstall hexo-renderer-marked --save</code><br><code>$ npm install hexo-renderer-kramed --save</code></p><p>그리고 <code>&lt;your-project-dir&gt;/node_modules/hexo-reneder-kramed/lib/renderer.js</code>를 열어 다음과 같이 return 값을 text로 수정합니다.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// Fit kramed's rule: $$ + \1 + $$</span></span><br><span class="line">  <span class="comment">// return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');</span></span><br><span class="line">  <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-mathjax-설치"><a href="#2-mathjax-설치" class="headerlink" title="2) mathjax 설치"></a>2) mathjax 설치</h3><p>다음으로는 mathjax plugin을 설치합니다.<br><code>npm install hexo-renderer-mathjax --save</code></p><p>그리고 <code>&lt;your-project-dir&gt;/node_modules/hexo-reneder-mathjax/mathjax.html</code>을 열고 CDN URL을 아래와 같이 수정합니다.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- &lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-LaTex와-markdown의-문법-충돌-fix하기"><a href="#2-LaTex와-markdown의-문법-충돌-fix하기" class="headerlink" title="2. LaTex와 markdown의 문법 충돌 fix하기"></a>2. LaTex와 markdown의 문법 충돌 fix하기</h2><p>LaTex와 markdown에는 다음과 같이 문법이 충돌하는 부분이 있습니다. </p><ul><li>markdown: <code>*</code>과 <code>_</code>는 bold와 italic</li><li>LaTex: <code>_</code>는 subscript</li></ul><p>따라서 <code>_</code>는 LaTex의 문법만을 따라서 아랫첨자를 나타내도록 하기위해<code>node_modules\kramed\lib\rules\inline.js</code>를 열고 다음과 같이 수정합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure><h2 id="3-Mathjax-사용하기"><a href="#3-Mathjax-사용하기" class="headerlink" title="3. Mathjax 사용하기"></a>3. Mathjax 사용하기</h2><p>사용하고 있는 theme의 <code>_config.yml</code> 파일을 열고 다음과 같이 mathjax를 enabling 해줍니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure><h2 id="4-markdown-post-작성하기"><a href="#4-markdown-post-작성하기" class="headerlink" title="4. markdown post 작성하기"></a>4. markdown post 작성하기</h2><p>이제 hexo 블로그에 수식을 사용하기 위한 설정은 모두 마쳤습니다.<br>마지막으로 post 작성시 header 부분에 <code>mathjax: true</code>를 넣어주면 블로그에 수식이 잘 표현되게 됩니다.</p><h4 id="참고-자료"><a href="#참고-자료" class="headerlink" title="참고 자료"></a>참고 자료</h4><ul><li>블로그 <a href="https://www.infiniteft.xyz/2018/03/21/make-hexo-support-latex/" target="_blank" rel="noopener">Make Hexo Support Latex</a></li><li>블로그 <a href="https://irongaea.github.io/2018/08/21/hexo-inline-math/" target="_blank" rel="noopener">hexo-inline-math</a></li><li>블로그 <a href="https://johngrib.github.io/wiki/mathjax-latex/#3-%EB%8F%84%EA%B5%AC" target="_blank" rel="noopener">MathJax로 LaTeX 사용하기</a></li><li><a href="https://www.mathjax.org/#gettingstarted" target="_blank" rel="noopener">www.mathjax.org</a></li></ul>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="HyeShin Oh" scheme="https://databuzz-team.github.io/categories/HyeShin-Oh/"/>
    
    
      <category term="Hexo" scheme="https://databuzz-team.github.io/tags/Hexo/"/>
    
      <category term="Mathjax" scheme="https://databuzz-team.github.io/tags/Mathjax/"/>
    
  </entry>
  
  <entry>
    <title>&lt;DataBuzz&gt; 블로그 작성법 및 주의사항</title>
    <link href="https://databuzz-team.github.io/2018/10/10/How-To-Use/"/>
    <id>https://databuzz-team.github.io/2018/10/10/How-To-Use/</id>
    <published>2018-10-10T09:04:34.000Z</published>
    <updated>2018-10-13T07:04:42.780Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Hexo-Scaffolds란"><a href="#Hexo-Scaffolds란" class="headerlink" title="Hexo Scaffolds란?"></a>Hexo Scaffolds란?</h3><p>hexo new -[page/post/draft 등] 을 실행했을 때 Default로 들어갈 정보를 입력하는 공간이다.</p><h3 id="1-Scaffolds-post-md-파일-수정하기"><a href="#1-Scaffolds-post-md-파일-수정하기" class="headerlink" title="1. Scaffolds/post.md 파일 수정하기"></a>1. Scaffolds/post.md 파일 수정하기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /scaffolds/post.md</span></span><br><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">categories:</span><br><span class="line">  - <span class="string">"Danial Nam"</span> &lt;- 이 부분을 자신의 이름으로 변경한다.</span><br><span class="line">tags:</span><br><span class="line">  -</span><br><span class="line">thumbnail:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h3 id="2-New-post-추가하기"><a href="#2-New-post-추가하기" class="headerlink" title="2. New post 추가하기"></a>2. New post 추가하기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new post <span class="string">"제목"</span></span><br></pre></td></tr></table></figure><h3 id="3-Title"><a href="#3-Title" class="headerlink" title="3. Title"></a>3. Title</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Title에는 []를 사용할 경우 오류가난다.</span><br><span class="line">필요시에는 &lt;&gt;로 대체할 것.</span><br></pre></td></tr></table></figure><h3 id="4-Tags"><a href="#4-Tags" class="headerlink" title="4. Tags"></a>4. Tags</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">깔끔한 블로그 관리를 위하여 Tag는 영어 Full Name으로 표기하며,</span><br><span class="line">파스칼케이스를 사용한다.</span><br><span class="line"></span><br><span class="line"><span class="comment"># Depth 및 Indent에 주의할 것!</span></span><br><span class="line">tags:</span><br><span class="line">  - Artificial Intelligence</span><br><span class="line">  - Data Science</span><br><span class="line">  -</span><br><span class="line"></span><br><span class="line">예)</span><br><span class="line">  AI -&gt; Artificial Intelligence</span><br><span class="line">  Data science -&gt; Data Science</span><br></pre></td></tr></table></figure><h3 id="5-Deploy-중요함"><a href="#5-Deploy-중요함" class="headerlink" title="5. Deploy(중요함!!)"></a>5. Deploy(중요함!!)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh post.sh</span><br><span class="line"><span class="comment"># 반드시 위의 명령을 실행하여 업로드한다!</span></span><br><span class="line"><span class="comment"># hexo g -d 명령을 사용해서 업로드해서는 안된다!!</span></span><br></pre></td></tr></table></figure><p>마지막에 Git pull해라는 오류가 뜨면, Pull을 실행하고 다시 sh post.sh로 재업로드해줄것!</p>]]></content>
    
    <!-- <summary type="html">
    
    </summary> -->
    
      <category term="Danial Nam" scheme="https://databuzz-team.github.io/categories/Danial-Nam/"/>
    
    
      <category term="Data Science" scheme="https://databuzz-team.github.io/tags/Data-Science/"/>
    
  </entry>
  
</feed>
