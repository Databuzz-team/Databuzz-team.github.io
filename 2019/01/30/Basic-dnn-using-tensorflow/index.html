<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Databuzz, Databuzz.team@gmail.com"><title><Deep Learning> Tensorflow로 DNN 모델링하며 Good Practice에 대해서 생각해보자 · Databuzz's Tech Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="description" content="Tensorflow로 DNN 모델링하며 Good Practice에 대해서 생각해보자
      
      
        이 컨텐츠는 DanTheTech 블로그로 옮겨졌습니다!
        불편을 끼쳐드려 죄송합니다. 이 링크를 클릭하셔서 확인해주시면 정말 감사하겠"><meta name="keywords" content="DataScience"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta content="hA9pHsXOdxxbLoAhWc2b_3tvhy-58l3vHmH8dw3WVkA" name="google-site-verification"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/danial/tensorflow.jpeg" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/brands.css" integrity="sha384-rf1bqOAj3+pw6NqYrtaE1/4Se2NBwkIfeYbsFdtiR6TQz0acWiwJbv1IM/Nt/ite" crossorigin="anonymous"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/reset.css"><link rel="stylesheet" href="/css/new.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Luckiest+Guy|Stylish|Boogaloo"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-127200932-1"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/custom.js"></script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/main_logo_new.png" style="width:127px;"><h3 class="sidebar_title" title="">DataBuzz</h3><div class="description"><p class="title_description">Awesome Tech Blog<a class="rss-icon-a" href="https://databuzz-team.github.io/atom" target="__blank"><i class='fas fa-rss rss-icon'></i></a></p></div></div></div><div class="featured-tags-wrap"><h1 class="featured-tags-title">FEATURED TAGS</h1><ul class="featured-tags-ul"><li class="featured-tags-li tag-li-detail"> Data Science </li><li class="featured-tags-li tag-li-detail"> Hexo </li><li class="featured-tags-li tag-li-detail"> Mathjax </li><li class="featured-tags-li tag-li-detail"> Tech News </li><li class="featured-tags-li tag-li-detail"> Git </li><li class="featured-tags-li tag-li-detail"> Machine Learning </li><li class="featured-tags-li tag-li-detail"> Artificial Intelligence </li><li class="featured-tags-li tag-li-detail"> Deep Learning </li><li class="featured-tags-li tag-li-detail"> Tensorflow </li><li class="featured-tags-li tag-li-detail"> Catboost </li><li class="featured-tags-li tag-li-detail"> Scikit Learn </li><li class="featured-tags-li tag-li-detail"> Hyperparameter </li><li class="featured-tags-li tag-li-detail"> Graph </li><li class="featured-tags-li tag-li-detail"> Session </li><li class="featured-tags-li tag-li-detail"> Markdown </li><li class="featured-tags-li tag-li-detail"> Regression </li><li class="featured-tags-li tag-li-detail"> Neural Network </li><li class="featured-tags-li tag-li-detail"> Portfolio </li><li class="featured-tags-li tag-li-detail"> Resume </li><li class="featured-tags-li tag-li-detail"> Back Propagation </li><li class="featured-tags-li tag-li-detail"> Deep Neural Networks </li></ul></div><div class="footer"><span>@ 2018 DataBuzz All Rights Reserved</span><br><span>Hexo Theme by </span><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><span>&nbsp;&nbsp;&nbsp; Edited by </span><a href="https://danialdaehyunnam.github.io"> Danial Nam </a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/"><i class="fas fa-home nav-icon"></i>Home</a></li><li><a href="/archives"><i class="fas fa-archive nav-icon"></i>Archives</a></li><li><a href="/tags-and-authors"><i class="fas fa-user-tag nav-icon"></i>Tags & Authors</a></li><li><a href="/about-us"><i class="fas fa-building nav-icon"></i>About Us</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left back-btn-icon" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/sun.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>&lt;Deep Learning&gt; Tensorflow로 DNN 모델링하며 Good Practice에 대해서 생각해보자</a></h3></div><div class="post-content post-detail"><div style="display: none;">
<!-- hexo-inject:begin --><!-- hexo-inject:end --><img src="/images/danial/tensorflow.jpeg">
</div>

<div class="danthetech-intro-wrap">
  <a class="danthetech-intro-a" href="https://danthetech.netlify.com/DataScience/basic-dnn-using-tensorflow/" target="_blank" rel="noopener">
    <img class="danthetech-img-wrap" src="/images/danial/tensorflow.jpeg">
    <div class="danthetech-p-wrap">
      <h1 class="danthetech-intro-title">
        Tensorflow로 DNN 모델링하며 Good Practice에 대해서 생각해보자
      </h1>
      <p class="danthetech-intro-p">
        <span class="danthetech-intro-strong">이 컨텐츠는 DanTheTech 블로그로 옮겨졌습니다!</span>
        불편을 끼쳐드려 죄송합니다. 이 링크를 클릭하셔서 확인해주시면 정말 감사하겠습니다.
        앞으로도 DataScience, App Development부터 DevOps관련 자료 등 도움될만한 글이 많이 올릴 예정이니 자주 들려주세요! :)
      </p>
    </div>
  </a>
</div>

<h3 id="About"><a href="#About" class="headerlink" title="About"></a>About</h3><p>이번 포스트에서는 Tensorflow를 이용하여 Deep Neural Networks를 구현하는 법을 간단히 알아보도록 하고, 어떻게 하면 코드 복사 붙여넣기 없이 할 수 있을까에 대해서 생각해보고 구현한 것을 공유하고자 한다.</p>
<p>특히 찾아보면 간단한 예제를 통해서 개념들을 설명하는 경우는 많지만 Practical한 예제를 사용한 경우는 드물어서 필자는 조금 더 Practical하게 작성하고자 노력해봤다.</p>
<p>다만 물론 필자도 경험이 많은 것이 아니어서, 아래의 예들이 좋은 코드 패턴은 아닐 수 있음에 양해를 구하며, 만약 더 좋은 생각이 나 궁금한 점은 댓글을 통해서 꼭 알려주시길 부탁드린다.</p>
<blockquote>
<p>만약 Neural Network에 대해서 잘 모르신다면 아래 링크들을 확인하시길<br>1.<a href="/2018/11/05/Back-Propagation/"><neural network=""> 인공신경망에 대한 이해(Part 1 - Feedforward Propagation)</neural></a><br>2.<a href="/2018/12/27/Back-Propagation-Part-2/"><neural network=""> 인공신경망에 대한 이해(Part 2 - Back Propagation)</neural></a></p>
</blockquote>
<h3 id="Tensorflow를-이용한-DNN-실습"><a href="#Tensorflow를-이용한-DNN-실습" class="headerlink" title="Tensorflow를 이용한 DNN 실습"></a>Tensorflow를 이용한 DNN 실습</h3><h4 id="연습으로-MNIST-Digit-이미지를-이용하도록-하자"><a href="#연습으로-MNIST-Digit-이미지를-이용하도록-하자" class="headerlink" title="연습으로 MNIST Digit 이미지를 이용하도록 하자."></a>연습으로 MNIST Digit 이미지를 이용하도록 하자.</h4><p>코드에서는 주석을 보며 생각흐름을 따라오면 된다.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"><span class="comment"># mnist dataset을 load한다.</span></span><br><span class="line">(x_train, y_train),(x_test, y_test) = mnist.load_data()</span><br><span class="line"><span class="comment"># float로 변환하고 minmax 스케일링을 한다. 이는 이미지 전처리의 가장 보편적인 방법 중 하나이다.</span></span><br><span class="line">x_train = x_train.reshape(<span class="number">60000</span>, <span class="number">784</span>).astype(<span class="string">'float32'</span>) / <span class="number">255.0</span></span><br><span class="line">x_test = x_test.reshape(<span class="number">10000</span>, <span class="number">784</span>).astype(<span class="string">'float32'</span>) / <span class="number">255.0</span></span><br><span class="line">print(x_train.shape, x_train.dtype)</span><br><span class="line"><span class="comment"># y 값을 one-hot-encoding로 변환해준다.</span></span><br><span class="line">y_unique_num = len(np.unique(y_train))</span><br><span class="line">y_train = np_utils.to_categorical(y_train, y_unique_num)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, y_unique_num)</span><br><span class="line">y_train[:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># test로 이미지를 한번 출력해보자.</span></span><br><span class="line">r = random.randint(<span class="number">0</span>, x_train.shape[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">plt.imshow(</span><br><span class="line">    x_train[r].reshape(<span class="number">28</span>, <span class="number">28</span>),</span><br><span class="line">    cmap=<span class="string">"Greys"</span>,</span><br><span class="line">    interpolation=<span class="string">"nearest"</span> <span class="comment"># 중간에 비어있는 값 처리</span></span><br><span class="line">)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h4 id="먼저-클래스를-사용하지-않고-구현해보자"><a href="#먼저-클래스를-사용하지-않고-구현해보자" class="headerlink" title="먼저 클래스를 사용하지 않고 구현해보자."></a>먼저 클래스를 사용하지 않고 구현해보자.</h4><p>아래는 Graph를 만드는 코드다.</p>
<p>혹시 placeholder, Variable 등 기본적인 함수에 대해서 잘 모른다면, <a href="https://databuzz-team.github.io/2018/10/24/Basic-deep-learning-tensorflow-for-beginner-2/"><deep learning=""> An introduction to deep learning with tensorflow(part-2)</deep></a> 블로그를 확인하면 된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input data를 위한 공간(placeholder)를 만든다.</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">28</span>*<span class="number">28</span>*<span class="number">1</span>])</span><br><span class="line"><span class="comment"># label data를 위한 공간도 만든다.</span></span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># layer 1</span></span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">28</span>*<span class="number">28</span>*<span class="number">1</span>, <span class="number">10</span>]))</span><br><span class="line">b1 = tf.Variable(tf.random_normal([<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># 이번 예제에서는 activation 함수로는 sigmoid를 사용하기로 하자.</span></span><br><span class="line">layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># layer 2</span></span><br><span class="line">W2 = tf.Variable(tf.random_normal([<span class="number">10</span>, <span class="number">20</span>]))</span><br><span class="line">b2 = tf.Variable(tf.random_normal([<span class="number">20</span>]))</span><br><span class="line">layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># layer 3</span></span><br><span class="line">W3 = tf.Variable(tf.random_normal([<span class="number">20</span>, <span class="number">20</span>]))</span><br><span class="line">b3 = tf.Variable(tf.random_normal([<span class="number">20</span>]))</span><br><span class="line">layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># layer 4</span></span><br><span class="line">W4 = tf.Variable(tf.random_normal([<span class="number">20</span>, <span class="number">10</span>]))</span><br><span class="line">b4 = tf.Variable(tf.random_normal([<span class="number">10</span>]))</span><br><span class="line">hypothesis = tf.nn.softmax(tf.matmul(layer3, W4) + b4)</span><br><span class="line"></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), axis=<span class="number">1</span>))</span><br><span class="line">train = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"></span><br><span class="line">prediction = tf.argmax(hypothesis, <span class="number">1</span>)</span><br><span class="line">is_correct = tf.equal(prediction, tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))</span><br></pre></td></tr></table></figure>
<p>그리고 세션을 이용해서 학습해보자.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">200</span></span><br><span class="line">epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(epoch):</span><br><span class="line">    total_batch = int(len(x_train)/batch_size)</span><br><span class="line">    c_avg = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">        batch_x = x_train[batch_size*i : batch_size*(i+<span class="number">1</span>)]</span><br><span class="line">        batch_y = y_train[batch_size*i : batch_size*(i+<span class="number">1</span>)]</span><br><span class="line">        c, _  = sess.run([cost, train], feed_dict=&#123;X: batch_x, y: batch_y&#125;)</span><br><span class="line">        c_avg = c_avg + (c/total_batch)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, c_avg)</span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;X: x_test, y: y_test&#125;))</span><br></pre></td></tr></table></figure></p>
<p>필자가 이 네트워크로 얻은 Accuracy값은 0.7291이었다. 그렇다면 이제 네트워크를 바꿔가며 하이퍼패러미터 튜닝을 시도해야할텐데, 그때마다 위의 Graph코드를 복사해서 붙여넣고 중간에 layer들은 변경한다거나 해야한다.</p>
<p>코드도 지저분해지고 자유도가 엄청 떨어지는 이 문제점을 해결하기 위해서 아래처럼 모델은 Class로 Train은 함수로 따로 구현해봤다.</p>
<p>코드가 많이 복잡해보이는데, 그 이유는 크게 4가지이다.</p>
<ol>
<li>Model은 Graph를 만드는 역할만 수행하고 Session과 결합하지 않았다.</li>
<li>Model을 빌드할 때 자유롭게 미리 config에서 설정한 layer, neuron의 개수, initializer, activation 등을 적용할 수 있게 하였다.</li>
<li>각 Layer마다 사용된 variable을 가져올 수 있게 하였다.</li>
<li>Tensorboard에도 기록될 수 있게 하였다.</li>
</ol>
<blockquote>
<p>이번 예제에서는 구현하지는 않았지만, activation이나 initializer 등을 넘어서 dropout 등도 응용해서 적용하면 된다.</p>
</blockquote>
<p>그렇게되면 장점은</p>
<ol>
<li>내부 layer 등을 달리한 모델 m1, m2를 객체화하고 학습은 같은 train() 함수를 이용해서 진행할 수 있어서 객체 내부에 중복된 train 함수를 들고있을 필요가 없다.</li>
<li>더 큰 네트워크를 만들기 용이하다.</li>
</ol>
<p>이제 코드로 살펴보자.</p>
<p>먼저 사용법을 살펴보고 나머지들을 설명하도록 하겠다.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input 데이터가 가진 feature 개수</span></span><br><span class="line">n_features = x_train.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># label 개수</span></span><br><span class="line">n_class = len(y_train[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># model build를 위한 config를 만든다.</span></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">"name"</span> : <span class="string">"dnn_model"</span>, <span class="comment"># 나중에 tensorboard를 확인하면 여기서 정한 이름으로 graph가 만들어진다.</span></span><br><span class="line">    <span class="string">"n_features"</span> : n_features,</span><br><span class="line">    <span class="string">"n_class"</span> : n_class,</span><br><span class="line">    <span class="string">"n_li"</span> : [n_features, <span class="number">1000</span>, <span class="number">1000</span>, <span class="number">1000</span>, n_class], <span class="comment"># input부터 output사이의 hidden layer neuron 개수들을 리스트형식으로 적어준다.</span></span><br><span class="line">    <span class="string">"initializer_li"</span> : [<span class="string">"random_normal"</span>, <span class="string">"random_normal"</span>, <span class="string">"random_normal"</span>, <span class="string">"random_normal"</span>], <span class="comment"># 각 레이어마다 Variable들이 사용할 initializer를 적어준다. 코드에서는 random_normal, xavier 두 가지 경우만을 고려하였다.</span></span><br><span class="line">    <span class="string">"activation_li"</span> : [<span class="string">"sigmoid"</span>, <span class="string">"sigmoid"</span>, <span class="string">"sigmoid"</span>, <span class="keyword">None</span>]</span><br><span class="line">    <span class="comment"># 각 레이어별로 뉴론에서 사용할 activation 함수를 적어준다. 코드에서는 sigmoid와 relu 두 가지 경우만을 고려하였다.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 객체를 만들자</span></span><br><span class="line">dnn_model = DNNModel(config)</span><br><span class="line"><span class="comment"># train함수에 만든 graph와 x_train, y_train을 넣어준다. epoch, lr, batch_size 등도 여기서 변경하며 실험해볼 수 있다.</span></span><br><span class="line">train(dnn_model, x_train, y_train, epoch=<span class="number">15</span>)</span><br><span class="line"><span class="comment"># accuracy, predict도 모델과 샘플 데이터들을 넣어주면 된다. 참고로 위에서 선언한 네트워크로 필자는 accuracy가 0.8로 나왔다.</span></span><br><span class="line">accuracy(dnn_model, x_test, y_test), predict(dnn_model, x_test)</span><br></pre></td></tr></table></figure></p>
<h4 id="Model-Class"><a href="#Model-Class" class="headerlink" title="Model Class"></a>Model Class</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        self.config = config <span class="comment"># 위에서 넣어준 config를 객체 내부에 저장하자.</span></span><br><span class="line">        self.endpoints = &#123;&#125; <span class="comment"># layer마다 사용한 variable을 저장할 공간을 만든다.</span></span><br><span class="line">        self.graph = tf.Graph() <span class="comment"># graph 정보를 train에서 session을 연결할 때 사용해야하므로 역시 객체에 저장해준다.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_net</span><span class="params">(self, x_placeholder, y_placeholder)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default(): <span class="comment"># 위에서 선언한 graph안에 빌드를 한다.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(self.config[<span class="string">"name"</span>]): <span class="comment"># tensorboard에서 확인하기 좋고, debugging에 유리하도록 name을 설정해준다.</span></span><br><span class="line">                self.X = x_placeholder <span class="comment"># 모델 클래스 자체가 blackbox처럼 만들기위해서 x_placeholder는 외부에서 주입받도록 하였다. input to output 매핑이 가능하도록..</span></span><br><span class="line">                self.y = y_placeholder <span class="comment"># 마찬가지로 위부(train 함수)에서 주입을 반든다.</span></span><br><span class="line"></span><br><span class="line">                layer_output_li = []</span><br><span class="line">                <span class="comment"># 항상 다음 layer에서 activation 함수를 통과할 때는 직전 layer에서 activation을 통과해서 나온 값과 현재 layer의 weight 및 bias와 연산을 진행하게된다.</span></span><br><span class="line">                <span class="comment"># 그러므로 각 layer output은 리스트로 저장해서 필요시 사용하도록 한다.</span></span><br><span class="line">                <span class="keyword">for</span> idx, n <span class="keyword">in</span> enumerate(self.config[<span class="string">"n_li"</span>][:<span class="number">-1</span>]):</span><br><span class="line">                    <span class="keyword">with</span> tf.name_scope(<span class="string">"Layer_"</span> + str(idx)) <span class="keyword">as</span> scope:</span><br><span class="line">                        previous_dim = self.config[<span class="string">"n_li"</span>][idx]</span><br><span class="line">                        next_dim = self.config[<span class="string">"n_li"</span>][idx + <span class="number">1</span>]</span><br><span class="line">                        <span class="comment"># 아래의 shape은 중간의 weights matrix와 bias의 shape를 위해 필요하다.</span></span><br><span class="line">                        shape = [previous_dim, next_dim]</span><br><span class="line">                        <span class="comment"># 이전 layer output을 가져오도록 하자.</span></span><br><span class="line">                        pre_layer_output = layer_output_li[<span class="number">-1</span>] <span class="keyword">if</span> idx &gt; <span class="number">0</span> <span class="keyword">else</span> self.X</span><br><span class="line">                        self.__set_weight_and_bias(idx, shape)</span><br><span class="line">                        layer = self.__set_layer_endpoint(idx, pre_layer_output)</span><br><span class="line">                        layer_output_li.append(layer)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.name_scope(<span class="string">"Cost"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">                self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y))</span><br><span class="line">                cost_sum = tf.summary.scalar(<span class="string">"Cost"</span>, self.cost)</span><br><span class="line"></span><br><span class="line">            self.predict = tf.argmax(self.logits, <span class="number">1</span>)</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(self.logits, <span class="number">1</span>), tf.argmax(self.y, <span class="number">1</span>))</span><br><span class="line">            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__set_weight_and_bias</span><span class="params">(self, idx, shape)</span>:</span></span><br><span class="line">        <span class="comment"># random_normal과 xavier만 만들었지만, 필요하면 if문을 추가하면된다.</span></span><br><span class="line">        <span class="keyword">if</span> self.config[<span class="string">"initializer_li"</span>][idx] == <span class="string">"random_normal"</span>:</span><br><span class="line">            self.endpoints[<span class="string">"W_"</span> + str(idx)] = tf.Variable(tf.random_normal(shape), name = <span class="string">"W_"</span> + str(idx))</span><br><span class="line">        <span class="keyword">elif</span> self.config[<span class="string">"initializer_li"</span>][idx] == <span class="string">"xavier"</span>:</span><br><span class="line">            self.endpoints[<span class="string">"W_"</span> + str(idx)] = tf.get_variable(<span class="string">"W_"</span> + str(idx), shape=shape,</span><br><span class="line">                                initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">        self.endpoints[<span class="string">"b_"</span> + str(idx)] = tf.Variable(tf.random_normal(shape[<span class="number">1</span>:]), name = <span class="string">"b_"</span> + str(idx))</span><br><span class="line">        W_hist = tf.summary.histogram(<span class="string">"W_hist_"</span> + str(idx), self.endpoints[<span class="string">"W_"</span> + str(idx)])</span><br><span class="line">        b_hist = tf.summary.histogram(<span class="string">"b_hist_"</span> + str(idx), self.endpoints[<span class="string">"b_"</span> + str(idx)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__set_layer_endpoint</span><span class="params">(self, idx, pre_layer_output)</span>:</span></span><br><span class="line">        W = self.endpoints[<span class="string">"W_"</span> + str(idx)]</span><br><span class="line">        b = self.endpoints[<span class="string">"b_"</span> + str(idx)]</span><br><span class="line">        <span class="keyword">if</span> idx + <span class="number">1</span> == len(self.config[<span class="string">"n_li"</span>][:<span class="number">-1</span>]):</span><br><span class="line">            self.logits = tf.matmul(pre_layer_output, W) + b</span><br><span class="line">            layer_hist = tf.summary.histogram(<span class="string">"Layer_hist_"</span> + str(idx), self.logits)</span><br><span class="line">            <span class="keyword">return</span> self.logits</span><br><span class="line"></span><br><span class="line">        <span class="comment"># weight &amp; bias와 마찬가지로 필요하면 sigmoid, relu 이외에도 추가하면 된다.</span></span><br><span class="line">        <span class="keyword">if</span> self.config[<span class="string">"activation_li"</span>][idx] == <span class="string">"sigmoid"</span>:</span><br><span class="line">            self.endpoints[<span class="string">"layer_"</span> + str(idx)] = tf.sigmoid(tf.matmul(pre_layer_output, W) + b)</span><br><span class="line">        <span class="keyword">elif</span> self.config[<span class="string">"activation_li"</span>][idx] == <span class="string">"relu"</span>:</span><br><span class="line">            self.endpoints[<span class="string">"layer_"</span> + str(idx)] = tf.nn.relu(tf.matmul(pre_layer_output, W) + b)</span><br><span class="line">        layer_hist = tf.summary.histogram(<span class="string">"Layer_hist_"</span> + str(idx), self.endpoints[<span class="string">"layer_"</span> + str(idx)])       </span><br><span class="line">        <span class="keyword">return</span> self.endpoints[<span class="string">"layer_"</span> + str(idx)]</span><br></pre></td></tr></table></figure>
<h4 id="Train-function"><a href="#Train-function" class="headerlink" title="Train function"></a>Train function</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, X_train, y_train, lr=<span class="number">1e-4</span>, epoch=<span class="number">15</span>, batch_size=<span class="number">200</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 모델의 그래프 안에 build하지 않으면 찾을 수 없다고 오류가 발생한다.</span></span><br><span class="line">    <span class="keyword">with</span> model.graph.as_default():</span><br><span class="line">        x_placeholder = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, model.config[<span class="string">"n_features"</span>]], name=<span class="string">"X"</span>)</span><br><span class="line">        y_placeholder = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, model.config[<span class="string">"n_class"</span>]], name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># graph에 build</span></span><br><span class="line">    model.build_net(x_placeholder, y_placeholder)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Session이 정확히 특정한 graph에 연결을 하기 때문에 각 객체간에 엇갈릴 일이 없다.</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=model.graph) <span class="keyword">as</span> sess:</span><br><span class="line">        train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(model.cost)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        merged_summary = tf.summary.merge_all()</span><br><span class="line">        writer = tf.summary.FileWriter(<span class="string">"./logs"</span>, sess.graph)</span><br><span class="line">        sess.run(init)</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(epoch):</span><br><span class="line">            total_batch = int(len(X_train)/batch_size)</span><br><span class="line">            c_avg = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">                batch_x = X_train[batch_size*i : batch_size*(i+<span class="number">1</span>)]</span><br><span class="line">                batch_y = y_train[batch_size*i : batch_size*(i+<span class="number">1</span>)]</span><br><span class="line">                summary, c, _  = sess.run([merged_summary, model.cost, train_op],</span><br><span class="line">                                              feed_dict=&#123;model.X: batch_x, model.y: batch_y&#125;)</span><br><span class="line">                c_avg = c_avg + (c/total_batch)</span><br><span class="line">                writer.add_summary(summary, i)</span><br><span class="line">            print(step, c_avg)</span><br><span class="line">        saver = tf.train.Saver()</span><br><span class="line">        <span class="comment"># predict나 accuracy도 model 밖에서 접근하므로 use uninitialized weights 오류를 피하려면 checkpoint를 저장하고 불러쓰는 방법을 써야했다.</span></span><br><span class="line">        saver.save(sess, <span class="string">'./checkpoint/'</span> + model.config[<span class="string">"name"</span>] + <span class="string">'.chkp'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Predict-and-accuracy-function"><a href="#Predict-and-accuracy-function" class="headerlink" title="Predict and accuracy function"></a>Predict and accuracy function</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, x_test)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=model.graph) <span class="keyword">as</span> sess:</span><br><span class="line">        saver = tf.train.Saver()</span><br><span class="line">        saver.restore(sess, <span class="string">'./checkpoint/'</span> + model.config[<span class="string">"name"</span>] + <span class="string">'.chkp'</span>)</span><br><span class="line">        <span class="keyword">return</span> sess.run([model.predict], feed_dict=&#123;model.X : x_test&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(model, x_test, y_test)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=model.graph) <span class="keyword">as</span> sess:</span><br><span class="line">        saver = tf.train.Saver()</span><br><span class="line">        saver.restore(sess, <span class="string">'./checkpoint/'</span> + model.config[<span class="string">"name"</span>] + <span class="string">'.chkp'</span>)</span><br><span class="line">        <span class="keyword">return</span> sess.run([model.accuracy], feed_dict=&#123;model.X : x_test, model.y : y_test&#125;)</span><br></pre></td></tr></table></figure>
<p>마지막으로 학습한 Endpoints를 확인하고 싶다면?<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnn_model.endpoints</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/danial/dnn/dnn_endpoints.png"></p>
<p>참고로 아래처럼 config를 하면 accuracy는 0.9749까지 올라간다.(이 network가 최고라는 것은 결코 아니니 오해하지 마시길)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_features = x_train.shape[<span class="number">1</span>]</span><br><span class="line">n_class = len(y_train[<span class="number">0</span>])</span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">"name"</span> : <span class="string">"dnn_model"</span>,</span><br><span class="line">    <span class="string">"n_features"</span> : n_features,</span><br><span class="line">    <span class="string">"n_class"</span> : n_class,</span><br><span class="line">    <span class="string">"n_li"</span> : [n_features, <span class="number">1000</span>, <span class="number">1000</span>, <span class="number">1000</span>, n_class],</span><br><span class="line">    <span class="string">"initializer_li"</span> : [<span class="string">"xavier"</span>, <span class="string">"xavier"</span>, <span class="string">"xavier"</span>, <span class="string">"xavier"</span>],</span><br><span class="line">    <span class="string">"activation_li"</span> : [<span class="string">"relu"</span>, <span class="string">"relu"</span>, <span class="string">"relu"</span>, <span class="keyword">None</span>]</span><br><span class="line">&#125;</span><br><span class="line">dnn_model = DNNModel(config)</span><br><span class="line">train(dnn_model, x_train, y_train, epoch=<span class="number">15</span>)</span><br></pre></td></tr></table></figure></p>
<p>전체 코드는 <a href="https://github.com/DanialDaeHyunNam/Deep-Learning-Good-Practice/tree/master/tensorflow/dnn" target="_blank" rel="noopener">github</a>에도 올려놨으니 필요하신분은 확인하시길..</p>
<p>위에서도 설명했지만 이 방법이 좋은 방법인지 필자도 알 수는 없다. 적어도 필자의 목적은 이룬 코드 패턴이어서 소개를 하였는데, 부디 도움이 되길 바란다.</p>
<div class="notebook-embedded">
<iframe src="https://nbviewer.jupyter.org/github/DanialDaeHyunNam/Deep-Learning-Good-Practice/blob/master/tensorflow/dnn/Tensorflow_DNN_example.ipynb" width="100%" height="100%" frameborder="0" allowfullscreen></iframe>
</div>

<h3 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h3><p><a href="https://danthetech.netlify.com/DataScience/basic-dnn-using-tensorflow/" target="_blank" rel="noopener"><deep learning=""> Tensorflow로 DNN 모델링하며 Good Practice에 대해서 생각해보자</deep></a></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date tag-not-hover">2019-01-30</span><i class="fa fa-comment-o"></i><a href="/2019/01/30/Basic-dnn-using-tensorflow/#comments">Comments</a><!-- i.fas.fa-pencil-alt--><!-- a.tag(href=config.root + item.path, title= item.name)!= "Written by " + '<img class="post-footer-author" src="/images/avatar/' + png_name + '.png"> ' + item.name + " "--><a class="tag tag-not-hover">Written by <img class="post-footer-author" src="/images/avatar/danial-nam.png"> Danial Nam </a><br><i class="fa fa-tag"></i><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Data Science </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Machine Learning </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Deep Learning </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Tensorflow </a><!-- a.tag(href=config.root + item.path, title= item.name)= "#" + item.name + " "--><a class="tag tag-not-hover">#Deep Neural Networks </a><div class="share-to-social">Share on <div class="sharer" id="sharer"> <div class="sharer-item" data-n="facebook"><i class="fab fa-facebook-f"></i></div><div class="sharer-item" data-n="twitter"><i class="fab fa-twitter"></i></div><div class="sharer-item" data-n="pinterest"><i class="fab fa-pinterest"></i></div></div></div></div></div></div></div><div class="share"></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2019/01/30/difference-between-graph-and-session/" title="&lt;Tensorflow&gt; Tensorflow에서 Graph와 Session의 차이">Next post</a></li></ul></div><a id="comments"></a><div id="disqus_thread"></div><script>var disqus_shortname = 'Databuzz';
var disqus_identifier = '2019/01/30/Basic-dnn-using-tensorflow/';
var disqus_title = '&lt;Deep Learning&gt; Tensorflow로 DNN 모델링하며 Good Practice에 대해서 생각해보자';
var disqus_url = 'https://databuzz-team.github.io/2019/01/30/Basic-dnn-using-tensorflow/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script></div></div></div></div><a href="#" id="back-to-top" title="Back to top"><i class="fas fa-angle-up"></i></a><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>